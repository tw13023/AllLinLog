{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook trains and evaluates AllLingLog on HDFS dataset. \n",
    "## Requirements\n",
    "\n",
    "1. `torch==2.7.1+cu128`\n",
    "2. `numpy==2.3.1`\n",
    "3. `pandas==2.3.1`\n",
    "4. `scikit-learn==1.7.0`\n",
    "5. `tqdm==4.67.1`\n",
    "6. `tiktoken==0.9.0`\n",
    "7. `linformer==0.2.3`\n",
    "8. `psutil==7.0.0`\n",
    "9. `matplotlib==3.10.3`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n",
      "Loading logs from: ./logs/HDFS.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Logs: 11175629it [00:05, 2111521.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11175629 logs in 5.29 seconds.\n",
      "Loading cl100k_base (GPT-4) tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping logs by session: 100%|██████████| 11175629/11175629 [00:57<00:00, 195088.63it/s]\n",
      "Processing sessions: 100%|██████████| 575061/575061 [04:09<00:00, 2305.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens in 90% of sessions: 1210\n",
      "Max tokens in 95% of sessions: 1362\n",
      "Max tokens in 100% of sessions: 15166\n",
      "Total unique session block IDs: 575061\n",
      "Total block IDs in label file: 575061\n",
      "Number of block IDs common to both: 575061\n",
      "Number of sessions: 575061. Train: 402542, Val: 86259, Test: 86260\n",
      "Train set => Normal: 390755 | Anomalous: 11787\n",
      "Anomalous to Total samples ratio in Train set: 0.0293\n",
      "Validation set => Normal: 83734 | Anomalous: 2525\n",
      "Anomalous to Total samples ratio in Validation set: 0.0293\n",
      "Test set => Normal: 83734 | Anomalous: 2526\n",
      "Anomalous to Total samples ratio in Test set: 0.0293\n",
      "Balancing training data with oversampling...\n",
      "Original Minority Samples: 11787\n",
      "New Target Minority Size: 260503\n",
      "New Total Samples: 651258\n",
      "New Anomalous to Total ratio: 0.40\n",
      "Balanced training data: 651258 samples\n",
      "Total processing time: 348.34 seconds.\n",
      "=========== Start of Validate Sessions =====================\n",
      "Number of sessions: 651258\n",
      "\n",
      "Session 1:\n",
      "Block ID: blk_2858264170709179290\n",
      "Input IDs: [27, 22534, 5120, 220, 11139, 11727, 220, 1758, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 69826, 4818, 25, 611, 882, 73174, 97078, 8754, 20205, 79190, 20205, 8366, 62, 1049, 22588, 25221, 14649, 62, 931, 18, 722, 62, 6726, 4119, 62, 16, 50081, 12, 11139, 1721, 13, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 100257, 22534, 5120, 220, 11139, 11727, 220, 22922, 15, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 2338, 25, 611, 605, 13, 13860, 13, 2148, 13, 6330, 25, 19608, 1954, 3281, 25, 611, 605, 13, 13860, 13, 2148, 13, 6330, 25, 2636, 605, 100257, 22534, 5120, 220, 11139, 11727, 220, 23388, 16, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 2338, 25, 611, 605, 13, 13860, 13, 2148, 13, 6330, 25, 19867, 843, 3281, 25, 611, 605, 13, 13860, 13, 2148, 13, 6330, 25, 2636, 605, 100257, 22534, 5120, 220, 11139, 11727, 220, 15894, 21, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 2338, 25, 611, 605, 13, 13860, 13, 1987, 13, 4331, 25, 18517, 1399, 3281, 25, 611, 605, 13, 13860, 13, 1987, 13, 4331, 25, 2636, 605, 100257, 22534, 5120, 220, 11139, 13384, 220, 1591, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 7461, 13, 1806, 25, 2636, 605, 374, 3779, 311, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 11139, 13384, 220, 966, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 2148, 13, 6330, 25, 2636, 605, 374, 3779, 311, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 11139, 13384, 220, 1644, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 1987, 13, 4331, 25, 2636, 605, 374, 3779, 311, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 11139, 13384, 220, 22922, 16, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 17, 369, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 71681, 100257, 22534, 5120, 220, 11139, 13384, 220, 22922, 16, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 2148, 13, 6330, 100257, 22534, 5120, 220, 11139, 13384, 220, 23388, 17, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 16, 369, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 71681, 100257, 22534, 5120, 220, 11139, 13384, 220, 23388, 17, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 2148, 13, 6330, 100257, 22534, 5120, 220, 11139, 13384, 220, 15894, 22, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 15, 369, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 71681, 100257, 22534, 5120, 220, 11139, 13384, 220, 15894, 22, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 1987, 13, 4331, 100257, 22534, 5120, 220, 11139, 24735, 220, 1682, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 13860, 13, 7461, 13, 1806, 25, 2636, 605, 100257, 22534, 5120, 220, 11139, 24735, 220, 1682, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 13860, 13, 2148, 13, 6330, 25, 2636, 605, 100257, 22534, 5120, 220, 11139, 24735, 220, 1682, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 13860, 13, 1987, 13, 4331, 25, 2636, 605, 100257, 22534, 5120, 220, 11139, 23486, 220, 777, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 843, 14, 36089, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 100257, 22534, 5120, 220, 11139, 20785, 220, 972, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 3226, 14, 36089, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 100257, 22534, 5120, 220, 11139, 11711, 220, 777, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 3391, 14, 36089, 62, 15935, 23038, 19561, 17819, 24391, 25344, 15, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18]\n",
      "Session Label: 0\n",
      "Decoded Log: <081110 020234 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_002001_1/part-02001. blk_2858264170709179290<|endoftext|>081110 020234 6130 INFO dfs.DataNode$DataXceiver: Receiving block blk_2858264170709179290 src: /10.251.31.160:46190 dest: /10.251.31.160:50010<|endoftext|>081110 020234 6191 INFO dfs.DataNode$DataXceiver: Receiving block blk_2858264170709179290 src: /10.251.31.160:37932 dest: /10.251.31.160:50010<|endoftext|>081110 020234 6256 INFO dfs.DataNode$DataXceiver: Receiving block blk_2858264170709179290 src: /10.251.38.53:42460 dest: /10.251.38.53:50010<|endoftext|>081110 020312 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.37:50010 is added to blk_2858264170709179290 size 67108864<|endoftext|>081110 020312 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.31.160:50010 is added to blk_2858264170709179290 size 67108864<|endoftext|>081110 020312 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.38.53:50010 is added to blk_2858264170709179290 size 67108864<|endoftext|>081110 020312 6131 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_2858264170709179290 terminating<|endoftext|>081110 020312 6131 INFO dfs.DataNode$PacketResponder: Received block blk_2858264170709179290 of size 67108864 from /10.251.31.160<|endoftext|>081110 020312 6192 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_2858264170709179290 terminating<|endoftext|>081110 020312 6192 INFO dfs.DataNode$PacketResponder: Received block blk_2858264170709179290 of size 67108864 from /10.251.31.160<|endoftext|>081110 020312 6257 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_2858264170709179290 terminating<|endoftext|>081110 020312 6257 INFO dfs.DataNode$PacketResponder: Received block blk_2858264170709179290 of size 67108864 from /10.251.38.53<|endoftext|>081110 020724 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2858264170709179290 is added to invalidSet of 10.251.106.37:50010<|endoftext|>081110 020724 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2858264170709179290 is added to invalidSet of 10.251.31.160:50010<|endoftext|>081110 020724 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2858264170709179290 is added to invalidSet of 10.251.38.53:50010<|endoftext|>081110 020727 19 INFO dfs.FSDataset: Deleting block blk_2858264170709179290 file /mnt/hadoop/dfs/data/current/subdir32/blk_2858264170709179290<|endoftext|>081110 020730 18 INFO dfs.FSDataset: Deleting block blk_2858264170709179290 file /mnt/hadoop/dfs/data/current/subdir57/blk_2858264170709179290<|endoftext|>081110 020750 19 INFO dfs.FSDataset: Deleting block blk_2858264170709179290 file /mnt/hadoop/dfs/data/current/subdir43/blk_2858264170709179290<|endoftext|>\n",
      "\n",
      "Session 2:\n",
      "Block ID: blk_8207318386620197032\n",
      "Input IDs: [27, 22534, 5120, 220, 6889, 12754, 220, 966, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 69826, 4818, 25, 611, 882, 73174, 97078, 20205, 79190, 20205, 8366, 62, 1049, 22588, 4645, 19592, 62, 931, 16, 722, 62, 931, 16368, 62, 15, 50081, 12, 931, 1032, 13, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 100257, 22534, 5120, 220, 6889, 12754, 220, 22744, 17, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 2338, 25, 611, 605, 13, 13860, 13, 6028, 13, 3534, 25, 10961, 2790, 3281, 25, 611, 605, 13, 13860, 13, 6028, 13, 3534, 25, 2636, 605, 100257, 22534, 5120, 220, 6889, 12754, 220, 24777, 21, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 2338, 25, 611, 605, 13, 13860, 13, 4513, 13, 1644, 25, 19081, 605, 3281, 25, 611, 605, 13, 13860, 13, 4513, 13, 1644, 25, 2636, 605, 100257, 22534, 5120, 220, 6889, 12754, 220, 7467, 15, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 2338, 25, 611, 605, 13, 13860, 13, 6028, 13, 3534, 25, 20385, 6281, 3281, 25, 611, 605, 13, 13860, 13, 6028, 13, 3534, 25, 2636, 605, 100257, 22534, 5120, 220, 6889, 15805, 220, 1591, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 5154, 13, 605, 13, 12533, 25, 2636, 605, 374, 3779, 311, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 6889, 15805, 220, 1758, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 6028, 13, 3534, 25, 2636, 605, 374, 3779, 311, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 6889, 15805, 220, 22744, 18, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 16, 369, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 71681, 100257, 22534, 5120, 220, 6889, 15805, 220, 22744, 18, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 6028, 13, 3534, 100257, 22534, 5120, 220, 6889, 15805, 220, 24777, 23, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 15, 369, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 71681, 100257, 22534, 5120, 220, 6889, 15805, 220, 24777, 23, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 4513, 13, 1644, 100257, 22534, 5120, 220, 6889, 15805, 220, 7467, 16, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 17, 369, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 71681, 100257, 22534, 5120, 220, 6889, 15805, 220, 7467, 16, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 6028, 13, 3534, 100257, 22534, 5120, 220, 6849, 16460, 220, 777, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 2610, 220, 605, 13, 5154, 13, 605, 13, 12533, 25, 2636, 605, 311, 46113, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 311, 3338, 276, 536, 1161, 8, 220, 605, 13, 13860, 13, 6028, 13, 2614, 25, 2636, 605, 100257, 22534, 5120, 220, 6849, 18517, 220, 777, 31871, 26877, 3417, 1997, 25, 220, 605, 13, 5154, 13, 605, 13, 12533, 25, 2636, 605, 28757, 4617, 311, 8481, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 311, 220, 605, 13, 13860, 13, 6028, 13, 2614, 25, 2636, 605, 100257, 22534, 5120, 220, 6849, 17837, 220, 22750, 22, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 2338, 25, 611, 605, 13, 5154, 13, 605, 13, 12533, 25, 21458, 6083, 3281, 25, 611, 605, 13, 5154, 13, 605, 13, 12533, 25, 2636, 605, 100257, 22534, 5120, 220, 6849, 19673, 220, 1644, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 6028, 13, 2614, 25, 2636, 605, 374, 3779, 311, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 6849, 19673, 220, 22750, 22, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 39517, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 2338, 25, 611, 605, 13, 5154, 13, 605, 13, 12533, 25, 21458, 6083, 3281, 25, 611, 605, 13, 5154, 13, 605, 13, 12533, 25, 2636, 605, 315, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 6849, 19673, 220, 22387, 18, 31871, 26877, 3417, 1997, 3, 1061, 22737, 25, 220, 605, 13, 5154, 13, 605, 13, 12533, 25, 2636, 605, 25, 3246, 5600, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 311, 611, 605, 13, 13860, 13, 6028, 13, 2614, 25, 2636, 605, 100257, 22534, 5120, 220, 6550, 25594, 220, 2148, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 4513, 13, 1644, 25, 2636, 605, 374, 3779, 311, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 6550, 22397, 220, 777, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 5495, 14, 36089, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 100257, 22534, 5120, 220, 8874, 16408, 220, 8011, 2705, 69609, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 6028, 13, 3534, 25, 2636, 605, 25, 33562, 4788, 1418, 13788, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 311, 611, 605, 13, 13860, 13, 2075, 13, 2491, 25, 100257, 22534, 5120, 220, 8874, 17837, 220, 8011, 5925, 69609, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 5154, 13, 605, 13, 12533, 25, 2636, 605, 25, 33562, 4788, 1418, 13788, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 311, 611, 605, 13, 13860, 13, 2075, 13, 2491, 25, 100257, 22534, 5120, 220, 8874, 20800, 220, 8011, 2589, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 6028, 13, 3534, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 311, 611, 605, 13, 13860, 13, 2075, 13, 2491, 100257, 22534, 5120, 220, 9756, 25326, 220, 1032, 31871, 26877, 3417, 4818, 32102, 25, 56573, 26399, 369, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 100257, 22534, 5120, 220, 8848, 10350, 220, 1591, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 5154, 13, 605, 13, 12533, 25, 2636, 605, 100257, 22534, 5120, 220, 8848, 10350, 220, 1591, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 13860, 13, 6028, 13, 2614, 25, 2636, 605, 100257, 22534, 5120, 220, 8848, 10350, 220, 1591, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 13860, 13, 6028, 13, 3534, 25, 2636, 605, 100257, 22534, 5120, 220, 8848, 20866, 220, 777, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 3174, 14, 36089, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 100257, 22534, 5120, 220, 8848, 24495, 220, 777, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 23, 14, 36089, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 100257, 22534, 5120, 220, 8848, 22194, 220, 777, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 22, 14, 36089, 62, 18248, 24626, 24250, 24199, 18089, 20436, 17, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29]\n",
      "Session Label: 1\n",
      "Decoded Log: <081110 103242 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_000013_0/part-00013. blk_8207318386620197032<|endoftext|>081110 103242 7152 INFO dfs.DataNode$DataXceiver: Receiving block blk_8207318386620197032 src: /10.251.71.97:45646 dest: /10.251.71.97:50010<|endoftext|>081110 103242 8976 INFO dfs.DataNode$DataXceiver: Receiving block blk_8207318386620197032 src: /10.251.123.33:38810 dest: /10.251.123.33:50010<|endoftext|>081110 103242 9000 INFO dfs.DataNode$DataXceiver: Receiving block blk_8207318386620197032 src: /10.251.71.97:44694 dest: /10.251.71.97:50010<|endoftext|>081110 103322 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.223:50010 is added to blk_8207318386620197032 size 67108864<|endoftext|>081110 103322 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.97:50010 is added to blk_8207318386620197032 size 67108864<|endoftext|>081110 103322 7153 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_8207318386620197032 terminating<|endoftext|>081110 103322 7153 INFO dfs.DataNode$PacketResponder: Received block blk_8207318386620197032 of size 67108864 from /10.251.71.97<|endoftext|>081110 103322 8978 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_8207318386620197032 terminating<|endoftext|>081110 103322 8978 INFO dfs.DataNode$PacketResponder: Received block blk_8207318386620197032 of size 67108864 from /10.251.123.33<|endoftext|>081110 103322 9001 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8207318386620197032 terminating<|endoftext|>081110 103322 9001 INFO dfs.DataNode$PacketResponder: Received block blk_8207318386620197032 of size 67108864 from /10.251.71.97<|endoftext|>081110 104422 19 INFO dfs.FSNamesystem: BLOCK* ask 10.250.10.223:50010 to replicate blk_8207318386620197032 to datanode(s) 10.251.71.68:50010<|endoftext|>081110 104424 19 INFO dfs.DataNode: 10.250.10.223:50010 Starting thread to transfer block blk_8207318386620197032 to 10.251.71.68:50010<|endoftext|>081110 104425 9127 INFO dfs.DataNode$DataXceiver: Receiving block blk_8207318386620197032 src: /10.250.10.223:52892 dest: /10.250.10.223:50010<|endoftext|>081110 104506 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.68:50010 is added to blk_8207318386620197032 size 67108864<|endoftext|>081110 104506 9127 INFO dfs.DataNode$DataXceiver: Received block blk_8207318386620197032 src: /10.250.10.223:52892 dest: /10.250.10.223:50010 of size 67108864<|endoftext|>081110 104506 9153 INFO dfs.DataNode$DataTransfer: 10.250.10.223:50010:Transmitted block blk_8207318386620197032 to /10.251.71.68:50010<|endoftext|>081110 105746 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.123.33:50010 is added to blk_8207318386620197032 size 67108864<|endoftext|>081110 105806 19 INFO dfs.FSDataset: Deleting block blk_8207318386620197032 file /mnt/hadoop/dfs/data/current/subdir63/blk_8207318386620197032<|endoftext|>081110 124405 11406 WARN dfs.DataNode$DataXceiver: 10.251.71.97:50010:Got exception while serving blk_8207318386620197032 to /10.251.75.49:<|endoftext|>081110 124425 11491 WARN dfs.DataNode$DataXceiver: 10.250.10.223:50010:Got exception while serving blk_8207318386620197032 to /10.251.75.49:<|endoftext|>081110 124447 11407 INFO dfs.DataNode$DataXceiver: 10.251.71.97:50010 Served block blk_8207318386620197032 to /10.251.75.49<|endoftext|>081110 152953 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_8207318386620197032<|endoftext|>081110 210138 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8207318386620197032 is added to invalidSet of 10.250.10.223:50010<|endoftext|>081110 210138 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8207318386620197032 is added to invalidSet of 10.251.71.68:50010<|endoftext|>081110 210138 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8207318386620197032 is added to invalidSet of 10.251.71.97:50010<|endoftext|>081110 210556 19 INFO dfs.FSDataset: Deleting block blk_8207318386620197032 file /mnt/hadoop/dfs/data/current/subdir41/blk_8207318386620197032<|endoftext|>081110 210638 19 INFO dfs.FSDataset: Deleting block blk_8207318386620197032 file /mnt/hadoop/dfs/data/current/subdir8/blk_8207318386620197032<|endoftext|>081110 210729 19 INFO dfs.FSDataset: Deleting block blk_8207318386620197032 file /mnt/hadoop/dfs/data/current/subdir7/blk_8207318386620197032<|endoftext|>\n",
      "\n",
      "Session 3:\n",
      "Block ID: blk_8072415692236325542\n",
      "Input IDs: [27, 22534, 5120, 220, 15901, 21125, 220, 1758, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 69826, 4818, 25, 611, 882, 73174, 97078, 8754, 20205, 79190, 20205, 8366, 62, 1049, 22588, 25221, 14649, 62, 931, 18, 722, 62, 4119, 13506, 62, 15, 50081, 12, 16037, 1135, 13, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 100257, 22534, 5120, 220, 15901, 21125, 220, 22915, 21, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 2338, 25, 611, 605, 13, 5154, 13, 777, 13, 845, 25, 22529, 2983, 3281, 25, 611, 605, 13, 5154, 13, 777, 13, 845, 25, 2636, 605, 100257, 22534, 5120, 220, 15901, 21125, 220, 22345, 15, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 2338, 25, 611, 605, 13, 5154, 13, 777, 13, 845, 25, 18517, 717, 3281, 25, 611, 605, 13, 5154, 13, 777, 13, 845, 25, 2636, 605, 100257, 22534, 5120, 220, 15901, 10617, 220, 20128, 18, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 2338, 25, 611, 605, 13, 13860, 13, 2983, 13, 12060, 25, 20304, 1187, 3281, 25, 611, 605, 13, 13860, 13, 2983, 13, 12060, 25, 2636, 605, 100257, 22534, 5120, 220, 15901, 17252, 220, 2148, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 5154, 13, 777, 13, 845, 25, 2636, 605, 374, 3779, 311, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 15901, 17252, 220, 2148, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 7461, 13, 1135, 25, 2636, 605, 374, 3779, 311, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 15901, 17252, 220, 1958, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 2983, 13, 12060, 25, 2636, 605, 374, 3779, 311, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 15901, 17252, 220, 22915, 22, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 16, 369, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 71681, 100257, 22534, 5120, 220, 15901, 17252, 220, 22915, 22, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 5154, 13, 777, 13, 845, 100257, 22534, 5120, 220, 15901, 17252, 220, 22345, 18, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 17, 369, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 71681, 100257, 22534, 5120, 220, 15901, 17252, 220, 22345, 18, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 5154, 13, 777, 13, 845, 100257, 22534, 5120, 220, 15901, 17252, 220, 20128, 20, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 15, 369, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 71681, 100257, 22534, 5120, 220, 15901, 17252, 220, 20128, 20, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 2983, 13, 12060, 100257, 22534, 5120, 220, 23545, 20823, 220, 21741, 19, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 2983, 13, 12060, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 311, 611, 605, 13, 13860, 13, 2983, 13, 12060, 100257, 22534, 5120, 220, 23545, 21729, 220, 25388, 22, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 5154, 13, 777, 13, 845, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 311, 611, 605, 13, 13860, 13, 914, 13, 14590, 100257, 22534, 5120, 220, 23545, 23644, 220, 18770, 21, 69609, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 7461, 13, 1135, 25, 2636, 605, 25, 33562, 4788, 1418, 13788, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 311, 611, 605, 13, 13860, 13, 7699, 13, 12754, 25, 100257, 22534, 5120, 220, 24508, 23582, 220, 23833, 17, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 7461, 13, 1135, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 311, 611, 605, 13, 13860, 13, 914, 13, 14590, 100257, 22534, 5120, 220, 24508, 18248, 220, 24376, 15, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 2983, 13, 12060, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 311, 611, 605, 13, 13860, 13, 2983, 13, 12060, 100257, 22534, 5120, 220, 24508, 26537, 220, 22148, 17, 69609, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 7461, 13, 1135, 25, 2636, 605, 25, 33562, 4788, 1418, 13788, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 311, 611, 605, 13, 13860, 13, 7699, 13, 12754, 25, 100257, 22534, 5120, 220, 24996, 19808, 220, 25465, 23, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 5154, 13, 777, 13, 845, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 311, 611, 605, 13, 13860, 13, 914, 13, 14590, 100257, 22534, 5120, 220, 24996, 18660, 220, 25016, 19, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 2983, 13, 12060, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 311, 611, 605, 13, 13860, 13, 2983, 13, 12060, 100257, 22534, 5120, 220, 24996, 23079, 220, 25125, 23, 69609, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 7461, 13, 1135, 25, 2636, 605, 25, 33562, 4788, 1418, 13788, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 311, 611, 605, 13, 13860, 13, 7699, 13, 12754, 25, 100257, 22534, 5120, 220, 6889, 23785, 220, 1958, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 5154, 13, 777, 13, 845, 25, 2636, 605, 100257, 22534, 5120, 220, 6889, 23785, 220, 1958, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 13860, 13, 7461, 13, 1135, 25, 2636, 605, 100257, 22534, 5120, 220, 6889, 23785, 220, 1958, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 13860, 13, 2983, 13, 12060, 25, 2636, 605, 100257, 22534, 5120, 220, 6889, 14735, 220, 777, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 5547, 14, 36089, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 100257, 22534, 5120, 220, 6889, 21860, 220, 777, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 1958, 14, 36089, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 100257, 22534, 5120, 220, 6889, 17973, 220, 777, 69609, 26877, 1006, 5608, 8534, 25, 71500, 1493, 4560, 311, 3783, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 13, 8527, 1767, 539, 1766, 304, 8286, 2276, 13, 100257, 22534, 5120, 220, 6889, 25505, 220, 777, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 1032, 14, 36089, 62, 23178, 13341, 23642, 12533, 20775, 22303, 17, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28]\n",
      "Session Label: 1\n",
      "Decoded Log: <081110 014449 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001550_0/part-01550. blk_8072415692236325542<|endoftext|>081110 014449 5786 INFO dfs.DataNode$DataXceiver: Receiving block blk_8072415692236325542 src: /10.250.19.16:57342 dest: /10.250.19.16:50010<|endoftext|>081110 014449 5860 INFO dfs.DataNode$DataXceiver: Receiving block blk_8072415692236325542 src: /10.250.19.16:42412 dest: /10.250.19.16:50010<|endoftext|>081110 014450 5923 INFO dfs.DataNode$DataXceiver: Receiving block blk_8072415692236325542 src: /10.251.42.207:46824 dest: /10.251.42.207:50010<|endoftext|>081110 014530 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.19.16:50010 is added to blk_8072415692236325542 size 67108864<|endoftext|>081110 014530 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.50:50010 is added to blk_8072415692236325542 size 67108864<|endoftext|>081110 014530 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.42.207:50010 is added to blk_8072415692236325542 size 67108864<|endoftext|>081110 014530 5787 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_8072415692236325542 terminating<|endoftext|>081110 014530 5787 INFO dfs.DataNode$PacketResponder: Received block blk_8072415692236325542 of size 67108864 from /10.250.19.16<|endoftext|>081110 014530 5863 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8072415692236325542 terminating<|endoftext|>081110 014530 5863 INFO dfs.DataNode$PacketResponder: Received block blk_8072415692236325542 of size 67108864 from /10.250.19.16<|endoftext|>081110 014530 5925 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_8072415692236325542 terminating<|endoftext|>081110 014530 5925 INFO dfs.DataNode$PacketResponder: Received block blk_8072415692236325542 of size 67108864 from /10.251.42.207<|endoftext|>081110 051531 6904 INFO dfs.DataNode$DataXceiver: 10.251.42.207:50010 Served block blk_8072415692236325542 to /10.251.42.207<|endoftext|>081110 051631 6977 INFO dfs.DataNode$DataXceiver: 10.250.19.16:50010 Served block blk_8072415692236325542 to /10.251.25.237<|endoftext|>081110 051752 7076 WARN dfs.DataNode$DataXceiver: 10.251.106.50:50010:Got exception while serving blk_8072415692236325542 to /10.251.107.242:<|endoftext|>081110 071815 7822 INFO dfs.DataNode$DataXceiver: 10.251.106.50:50010 Served block blk_8072415692236325542 to /10.251.25.237<|endoftext|>081110 071820 7620 INFO dfs.DataNode$DataXceiver: 10.251.42.207:50010 Served block blk_8072415692236325542 to /10.251.42.207<|endoftext|>081110 071849 7842 WARN dfs.DataNode$DataXceiver: 10.251.106.50:50010:Got exception while serving blk_8072415692236325542 to /10.251.107.242:<|endoftext|>081110 082622 8398 INFO dfs.DataNode$DataXceiver: 10.250.19.16:50010 Served block blk_8072415692236325542 to /10.251.25.237<|endoftext|>081110 082630 8244 INFO dfs.DataNode$DataXceiver: 10.251.42.207:50010 Served block blk_8072415692236325542 to /10.251.42.207<|endoftext|>081110 082639 8478 WARN dfs.DataNode$DataXceiver: 10.251.106.50:50010:Got exception while serving blk_8072415692236325542 to /10.251.107.242:<|endoftext|>081110 103045 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8072415692236325542 is added to invalidSet of 10.250.19.16:50010<|endoftext|>081110 103045 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8072415692236325542 is added to invalidSet of 10.251.106.50:50010<|endoftext|>081110 103045 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8072415692236325542 is added to invalidSet of 10.251.42.207:50010<|endoftext|>081110 103249 19 INFO dfs.FSDataset: Deleting block blk_8072415692236325542 file /mnt/hadoop/dfs/data/current/subdir61/blk_8072415692236325542<|endoftext|>081110 103551 19 INFO dfs.FSDataset: Deleting block blk_8072415692236325542 file /mnt/hadoop/dfs/data/current/subdir34/blk_8072415692236325542<|endoftext|>081110 103801 19 WARN dfs.FSDataset: Unexpected error trying to delete block blk_8072415692236325542. BlockInfo not found in volumeMap.<|endoftext|>081110 103856 19 INFO dfs.FSDataset: Deleting block blk_8072415692236325542 file /mnt/hadoop/dfs/data/current/subdir13/blk_8072415692236325542<|endoftext|>\n",
      "\n",
      "Session 4:\n",
      "Block ID: blk_6096578195065914060\n",
      "Input IDs: [27, 22534, 5120, 220, 16037, 21164, 220, 966, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 69826, 4818, 25, 611, 882, 73174, 97078, 8754, 20205, 79190, 20205, 8366, 62, 1049, 22588, 25221, 14649, 62, 931, 18, 722, 62, 4119, 23402, 62, 15, 50081, 12, 16037, 5728, 13, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 100257, 22534, 5120, 220, 16037, 21164, 220, 21228, 20, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 2338, 25, 611, 605, 13, 13860, 13, 2366, 13, 12652, 25, 17153, 2946, 3281, 25, 611, 605, 13, 13860, 13, 2366, 13, 12652, 25, 2636, 605, 100257, 22534, 5120, 220, 16037, 21164, 220, 18262, 18, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 2338, 25, 611, 605, 13, 13860, 13, 7461, 13, 605, 25, 23428, 2437, 3281, 25, 611, 605, 13, 13860, 13, 7461, 13, 605, 25, 2636, 605, 100257, 22534, 5120, 220, 16037, 21164, 220, 20354, 22, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 2338, 25, 611, 605, 13, 13860, 13, 2366, 13, 12652, 25, 13506, 1774, 3281, 25, 611, 605, 13, 13860, 13, 2366, 13, 12652, 25, 2636, 605, 100257, 22534, 5120, 220, 16037, 21791, 220, 1627, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 2031, 13, 1806, 25, 2636, 605, 374, 3779, 311, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 16037, 21791, 220, 1544, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 2366, 13, 12652, 25, 2636, 605, 374, 3779, 311, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 16037, 21791, 220, 1644, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 7461, 13, 605, 25, 2636, 605, 374, 3779, 311, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 16037, 21791, 220, 21228, 21, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 16, 369, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 71681, 100257, 22534, 5120, 220, 16037, 21791, 220, 21228, 21, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 2366, 13, 12652, 100257, 22534, 5120, 220, 16037, 21791, 220, 18262, 19, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 15, 369, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 71681, 100257, 22534, 5120, 220, 16037, 21791, 220, 18262, 19, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 7461, 13, 605, 100257, 22534, 5120, 220, 16037, 21791, 220, 20354, 23, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 17, 369, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 71681, 100257, 22534, 5120, 220, 16037, 21791, 220, 20354, 23, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 2366, 13, 12652, 100257, 22534, 5120, 220, 18070, 20465, 220, 1032, 31871, 26877, 3417, 4818, 32102, 25, 56573, 26399, 369, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 100257, 22534, 5120, 220, 23545, 22716, 220, 19597, 15, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 2366, 13, 12652, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 311, 611, 605, 13, 5154, 13, 605, 13, 1041, 100257, 22534, 5120, 220, 24130, 8874, 220, 23403, 17, 69609, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 7461, 13, 605, 25, 2636, 605, 25, 33562, 4788, 1418, 13788, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 311, 611, 605, 13, 13860, 13, 6804, 13, 2618, 25, 100257, 22534, 5120, 220, 23324, 19899, 220, 24054, 17, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 2366, 13, 12652, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 311, 611, 605, 13, 13860, 13, 11584, 13, 5894, 100257, 22534, 5120, 220, 24508, 22393, 220, 24876, 22, 69609, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 2031, 13, 1806, 25, 2636, 605, 25, 33562, 4788, 1418, 13788, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 311, 611, 605, 13, 13860, 13, 6804, 13, 2618, 25, 100257, 22534, 5120, 220, 24508, 21056, 220, 23901, 21, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 7461, 13, 605, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 311, 611, 605, 13, 5154, 13, 605, 13, 1041, 100257, 22534, 5120, 220, 23439, 19944, 220, 24763, 22, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 2366, 13, 12652, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 311, 611, 605, 13, 13860, 13, 2366, 13, 12652, 100257, 22534, 5120, 220, 23439, 22922, 220, 24531, 16, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 2366, 13, 12652, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 311, 611, 605, 13, 13860, 13, 7285, 13, 10697, 100257, 22534, 5120, 220, 24996, 22874, 220, 21474, 22, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 7461, 13, 605, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 311, 611, 605, 13, 5154, 13, 605, 13, 1041, 100257, 22534, 5120, 220, 24996, 21982, 220, 24650, 24, 69609, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 2366, 13, 12652, 25, 2636, 605, 25, 33562, 4788, 1418, 13788, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 311, 611, 605, 13, 13860, 13, 2366, 13, 12652, 25, 100257, 22534, 5120, 220, 25077, 17264, 220, 18670, 19, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 220, 605, 13, 13860, 13, 2366, 13, 12652, 25, 2636, 605, 328, 2841, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 311, 611, 605, 13, 13860, 13, 2366, 13, 12652, 100257, 22534, 5120, 220, 6889, 23785, 220, 1958, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 13860, 13, 7461, 13, 605, 25, 2636, 605, 100257, 22534, 5120, 220, 6889, 23785, 220, 1958, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 13860, 13, 2366, 13, 12652, 25, 2636, 605, 100257, 22534, 5120, 220, 6889, 23785, 220, 1958, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 7592, 25, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 374, 3779, 311, 8482, 1681, 315, 220, 605, 13, 13860, 13, 2031, 13, 1806, 25, 2636, 605, 100257, 22534, 5120, 220, 6889, 23024, 220, 972, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 2983, 14, 36089, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 100257, 22534, 5120, 220, 6889, 23969, 220, 777, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 605, 14, 36089, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 100257, 22534, 5120, 220, 6849, 24239, 220, 1544, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 923, 94343, 4818, 1715, 4036, 369, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 389, 220, 605, 13, 13860, 13, 7461, 13, 605, 25, 2636, 605, 1404, 220, 23403, 25620, 1227, 2030, 433, 1587, 539, 9352, 311, 904, 1052, 13, 100257, 22534, 5120, 220, 6849, 24239, 220, 1544, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 7461, 13, 605, 25, 2636, 605, 374, 3779, 311, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5120, 220, 6550, 15633, 220, 777, 31871, 26877, 1006, 5608, 8534, 25, 82950, 2565, 41743, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 1052, 611, 41982, 7682, 26335, 14, 35478, 13469, 75153, 38985, 3826, 975, 14, 36089, 62, 21138, 23480, 18831, 19673, 24380, 17264, 15, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "Session Label: 1\n",
      "Decoded Log: <081110 015513 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001574_0/part-01574. blk_6096578195065914060<|endoftext|>081110 015513 5755 INFO dfs.DataNode$DataXceiver: Receiving block blk_6096578195065914060 src: /10.251.202.209:33459 dest: /10.251.202.209:50010<|endoftext|>081110 015513 6013 INFO dfs.DataNode$DataXceiver: Receiving block blk_6096578195065914060 src: /10.251.106.10:59402 dest: /10.251.106.10:50010<|endoftext|>081110 015513 6047 INFO dfs.DataNode$DataXceiver: Receiving block blk_6096578195065914060 src: /10.251.202.209:55045 dest: /10.251.202.209:50010<|endoftext|>081110 015542 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.70.37:50010 is added to blk_6096578195065914060 size 67108864<|endoftext|>081110 015542 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.202.209:50010 is added to blk_6096578195065914060 size 67108864<|endoftext|>081110 015542 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.10:50010 is added to blk_6096578195065914060 size 67108864<|endoftext|>081110 015542 5756 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_6096578195065914060 terminating<|endoftext|>081110 015542 5756 INFO dfs.DataNode$PacketResponder: Received block blk_6096578195065914060 of size 67108864 from /10.251.202.209<|endoftext|>081110 015542 6014 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_6096578195065914060 terminating<|endoftext|>081110 015542 6014 INFO dfs.DataNode$PacketResponder: Received block blk_6096578195065914060 of size 67108864 from /10.251.106.10<|endoftext|>081110 015542 6048 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_6096578195065914060 terminating<|endoftext|>081110 015542 6048 INFO dfs.DataNode$PacketResponder: Received block blk_6096578195065914060 of size 67108864 from /10.251.202.209<|endoftext|>081110 025427 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_6096578195065914060<|endoftext|>081110 051828 7010 INFO dfs.DataNode$DataXceiver: 10.251.202.209:50010 Served block blk_6096578195065914060 to /10.250.10.100<|endoftext|>081110 052124 6712 WARN dfs.DataNode$DataXceiver: 10.251.106.10:50010:Got exception while serving blk_6096578195065914060 to /10.251.127.47:<|endoftext|>081110 061840 7282 INFO dfs.DataNode$DataXceiver: 10.251.202.209:50010 Served block blk_6096578195065914060 to /10.251.214.130<|endoftext|>081110 071905 7717 WARN dfs.DataNode$DataXceiver: 10.251.70.37:50010:Got exception while serving blk_6096578195065914060 to /10.251.127.47:<|endoftext|>081110 071910 7456 INFO dfs.DataNode$DataXceiver: 10.251.106.10:50010 Served block blk_6096578195065914060 to /10.250.10.100<|endoftext|>081110 072608 7927 INFO dfs.DataNode$DataXceiver: 10.251.202.209:50010 Served block blk_6096578195065914060 to /10.251.202.209<|endoftext|>081110 072613 7931 INFO dfs.DataNode$DataXceiver: 10.251.202.209:50010 Served block blk_6096578195065914060 to /10.251.193.224<|endoftext|>081110 082709 8097 INFO dfs.DataNode$DataXceiver: 10.251.106.10:50010 Served block blk_6096578195065914060 to /10.250.10.100<|endoftext|>081110 082718 8459 WARN dfs.DataNode$DataXceiver: 10.251.202.209:50010:Got exception while serving blk_6096578195065914060 to /10.251.202.209:<|endoftext|>081110 083406 8604 INFO dfs.DataNode$DataXceiver: 10.251.202.209:50010 Served block blk_6096578195065914060 to /10.251.202.209<|endoftext|>081110 103045 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6096578195065914060 is added to invalidSet of 10.251.106.10:50010<|endoftext|>081110 103045 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6096578195065914060 is added to invalidSet of 10.251.202.209:50010<|endoftext|>081110 103045 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6096578195065914060 is added to invalidSet of 10.251.70.37:50010<|endoftext|>081110 103722 18 INFO dfs.FSDataset: Deleting block blk_6096578195065914060 file /mnt/hadoop/dfs/data/current/subdir42/blk_6096578195065914060<|endoftext|>081110 103736 19 INFO dfs.FSDataset: Deleting block blk_6096578195065914060 file /mnt/hadoop/dfs/data/current/subdir10/blk_6096578195065914060<|endoftext|>081110 104629 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_6096578195065914060 on 10.251.106.10:50010 size 67108864 But it does not belong to any file.<|endoftext|>081110 104629 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.10:50010 is added to blk_6096578195065914060 size 67108864<|endoftext|>081110 105510 19 INFO dfs.FSDataset: Deleting block blk_6096578195065914060 file /mnt/hadoop/dfs/data/current/subdir14/blk_6096578195065914060<|endoftext|>\n",
      "\n",
      "Session 5:\n",
      "Block ID: blk_1404180427224131069\n",
      "Input IDs: [27, 22534, 5037, 220, 20945, 25594, 220, 7285, 4044, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 2338, 25, 611, 605, 13, 13860, 13, 1758, 13, 16, 25, 19498, 845, 3281, 25, 611, 605, 13, 13860, 13, 1758, 13, 16, 25, 2636, 605, 100257, 22534, 5037, 220, 20945, 25594, 220, 6280, 5313, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 2338, 25, 611, 605, 13, 13860, 13, 6549, 13, 7285, 25, 20615, 2421, 3281, 25, 611, 605, 13, 13860, 13, 6549, 13, 7285, 25, 2636, 605, 100257, 22534, 5037, 220, 20945, 25594, 220, 6280, 2421, 31871, 26877, 3417, 1997, 3, 1061, 55, 13158, 25, 1050, 47444, 2565, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 2338, 25, 611, 605, 13, 13860, 13, 6549, 13, 7285, 25, 21312, 1987, 3281, 25, 611, 605, 13, 13860, 13, 6549, 13, 7285, 25, 2636, 605, 100257, 22534, 5037, 220, 20945, 25594, 220, 1627, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 69826, 4818, 25, 611, 882, 73174, 97078, 8754, 19, 20205, 79190, 20205, 8366, 62, 1049, 22588, 4645, 19592, 62, 4119, 15, 722, 62, 4119, 11727, 62, 15, 50081, 12, 11531, 1958, 13, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 100257, 22534, 5037, 220, 20945, 25528, 220, 7285, 2421, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 15, 369, 2565, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 71681, 100257, 22534, 5037, 220, 20945, 25528, 220, 7285, 2421, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 1758, 13, 16, 100257, 22534, 5037, 220, 20945, 25528, 220, 6280, 2421, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 16, 369, 2565, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 71681, 100257, 22534, 5037, 220, 20945, 25528, 220, 6280, 2421, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 6549, 13, 7285, 100257, 22534, 5037, 220, 20945, 25528, 220, 6280, 4578, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 29989, 31984, 220, 17, 369, 2565, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 71681, 100257, 22534, 5037, 220, 20945, 25528, 220, 6280, 4578, 31871, 26877, 3417, 1997, 3, 17093, 31984, 25, 39517, 2565, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 315, 1404, 220, 23403, 25620, 1227, 505, 611, 605, 13, 13860, 13, 6549, 13, 7285, 100257, 22534, 5037, 220, 20945, 25528, 220, 1591, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 9639, 13, 19, 25, 2636, 605, 374, 3779, 311, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5037, 220, 20945, 25528, 220, 966, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 1758, 13, 16, 25, 2636, 605, 374, 3779, 311, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 1404, 220, 23403, 25620, 1227, 100257, 22534, 5037, 220, 20945, 25528, 220, 2148, 31871, 26877, 1006, 50, 8139, 615, 25, 29777, 9, 4076, 2374, 1388, 94343, 4818, 25, 2565, 2276, 6177, 25, 220, 605, 13, 13860, 13, 6549, 13, 7285, 25, 2636, 605, 374, 3779, 311, 41743, 62, 6860, 19770, 22349, 23024, 19288, 7461, 24, 1404, 220, 23403, 25620, 1227, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "Session Label: 0\n",
      "Decoded Log: <081111 041746 19387 INFO dfs.DataNode$DataXceiver: Receiving block blk_1404180427224131069 src: /10.251.35.1:39516 dest: /10.251.35.1:50010<|endoftext|>081111 041746 19585 INFO dfs.DataNode$DataXceiver: Receiving block blk_1404180427224131069 src: /10.251.125.193:59088 dest: /10.251.125.193:50010<|endoftext|>081111 041746 19588 INFO dfs.DataNode$DataXceiver: Receiving block blk_1404180427224131069 src: /10.251.125.193:51838 dest: /10.251.125.193:50010<|endoftext|>081111 041746 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt4/_temporary/_task_200811101024_0010_m_001234_0/part-01234. blk_1404180427224131069<|endoftext|>081111 041817 19388 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_1404180427224131069 terminating<|endoftext|>081111 041817 19388 INFO dfs.DataNode$PacketResponder: Received block blk_1404180427224131069 of size 67108864 from /10.251.35.1<|endoftext|>081111 041817 19588 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_1404180427224131069 terminating<|endoftext|>081111 041817 19588 INFO dfs.DataNode$PacketResponder: Received block blk_1404180427224131069 of size 67108864 from /10.251.125.193<|endoftext|>081111 041817 19589 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_1404180427224131069 terminating<|endoftext|>081111 041817 19589 INFO dfs.DataNode$PacketResponder: Received block blk_1404180427224131069 of size 67108864 from /10.251.125.193<|endoftext|>081111 041817 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.203.4:50010 is added to blk_1404180427224131069 size 67108864<|endoftext|>081111 041817 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.35.1:50010 is added to blk_1404180427224131069 size 67108864<|endoftext|>081111 041817 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.125.193:50010 is added to blk_1404180427224131069 size 67108864<|endoftext|>\n",
      "Average tokens per session: 886.45\n",
      "Max tokens in a session: 15166\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "# This cell read the public log dataset from /logs/HDFS.log.\n",
    "# Tokenizing the logs and creating input features for the model.\n",
    "# Input features was splitted as test set (70%), validation set (15%) \n",
    "# and test set (15%)\n",
    "##########################################################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score as f1_score_sklearn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re \n",
    "import tiktoken\n",
    "import random\n",
    "\n",
    "beta = 0.4\n",
    "\n",
    "max_token_length = 18000 #set an initial value for the max token length\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "\n",
    "class LogDataset(Dataset):\n",
    "    def __init__(self, sessions):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with a list of session dictionaries.\n",
    "        \n",
    "        Each session dictionary is expected to contain the keys:\n",
    "          - 'block_id'\n",
    "          - 'input_ids'\n",
    "          - 'segment_ids'\n",
    "          - 'session_label'\n",
    "        \"\"\"\n",
    "        self.sessions = sessions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        session = self.sessions[idx]\n",
    "        # Return a dictionary containing only the required three items.\n",
    "        compact_session = {\n",
    "            'input_ids': session['input_ids'],\n",
    "            'segment_ids': session['segment_ids'],\n",
    "            'session_label': session['session_label']\n",
    "        }\n",
    "        return compact_session\n",
    "\n",
    "# --- Helper Functions and Tokenizer Loading ---\n",
    "\n",
    "def clean(text):\n",
    "    \"\"\"Cleans a log message by removing special characters and extra spaces.\"\"\"\n",
    "    import re\n",
    "    import string\n",
    "    text = re.sub(r'\\]|\\[|\\)|\\(|\\=|\\,|\\;', ' ', text)\n",
    "    text = \" \".join([word.lower() if word.isupper() else word for word in text.strip().split()])\n",
    "    text = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', text))\n",
    "    return \" \".join([word.lower().strip() for word in text.strip().split()])\n",
    "\n",
    "# function to load the pre-trained GPT4 BPE tokenizer \n",
    "def load_gpt4_tokenizer():\n",
    "    print(\"Loading cl100k_base (GPT-4) tokenizer...\")\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return tokenizer\n",
    "\n",
    "# Tokenize logs with the pre-trained GPT-4 tokenizer \n",
    "def tokenize_and_construct_input(log_sequence, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Tokenize log messages and construct input IDs and segment IDs.\n",
    "    Adds beginning-of-sequence (bos) and end-of-sequence (eos) tokens, and assigns\n",
    "    segment IDs based on the log position.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    segment_ids = []\n",
    "\n",
    "    allowed_special = {\"<|startoftext|>\", \"<|endoftext|>\"}\n",
    "    bos_token = tokenizer.encode(\"<|startoftext|>\", allowed_special=allowed_special)[0] # get the token id of <|startoftext|>\n",
    "    eos_token = tokenizer.encode(\"<|endoftext|>\", allowed_special=allowed_special)[0] # get the token id of <|endoftext|>\n",
    "    \n",
    "    for i, log in enumerate(log_sequence):\n",
    "        # Tokenize the log message\n",
    "        tokens = tokenizer.encode(log, allowed_special={\"<|startoftext|>\", \"<|endoftext|>\"})\n",
    "        \n",
    "        # Add bos_token to the first log only in the sequence\n",
    "        if i == 0:  # First log in sequence\n",
    "            tokens = [bos_token] + tokens  # <|startoftext|> + log tokens\n",
    "        tokens = tokens + [eos_token]  # <|endoftext|> appended to the log tokens\n",
    "        \n",
    "        input_ids.extend(tokens)\n",
    "        segment_ids.extend([i] * len(tokens))\n",
    "    \n",
    "    if len(input_ids) > max_len:\n",
    "        print(f\"Warning: Sequence length {len(input_ids)} exceeds max length {max_len}. Truncating.\")\n",
    "        input_ids = input_ids[:max_len]\n",
    "        segment_ids = segment_ids[:max_len]\n",
    "    \n",
    "    return input_ids, segment_ids\n",
    "\n",
    "# --- Modified Oversampling Function ---\n",
    "def oversample_with_ratio_control(sessions, beta):\n",
    "    \"\"\"\n",
    "    Oversample the minority class to achieve a specific proportion (beta) in the dataset.\n",
    "    If no anomalous sessions are found, a warning is printed and the original sessions are returned.\n",
    "    \"\"\"\n",
    "    normal_sessions = [s for s in sessions if s[\"session_label\"] == 0]\n",
    "    anomalous_sessions = [s for s in sessions if s[\"session_label\"] == 1]\n",
    "    \n",
    "    if len(anomalous_sessions) == 0:\n",
    "        print(\"Warning: No anomalous sessions found. Skipping oversampling.\")\n",
    "        return sessions  # Return the sessions unchanged\n",
    "    \n",
    "    normal_count = len(normal_sessions)\n",
    "    anomalous_count = len(anomalous_sessions)\n",
    "    total_count = normal_count + anomalous_count\n",
    "    \n",
    "    alpha = anomalous_count / total_count\n",
    "    target_minority_size = int((beta * (1 - alpha) / (1 - beta)) * total_count)\n",
    "    new_added_anomalous = target_minority_size - anomalous_count\n",
    "\n",
    "    if target_minority_size <= anomalous_count:\n",
    "        print(\"No oversampling needed. Current minority size meets the target proportion.\")\n",
    "        return sessions\n",
    "\n",
    "    # Oversample anomalous sessions with replacement\n",
    "    new_added_anomalous = np.random.choice(anomalous_sessions, size=new_added_anomalous, replace=True)\n",
    "    oversampled_sessions = normal_sessions + anomalous_sessions + list(new_added_anomalous)\n",
    "    final_anomalous_count = len(anomalous_sessions) + len(new_added_anomalous)\n",
    "    np.random.shuffle(oversampled_sessions)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\"Original Minority Samples: {anomalous_count}\")\n",
    "    print(f\"New Target Minority Size: {target_minority_size}\")\n",
    "    print(f\"New Total Samples: {len(oversampled_sessions)}\")\n",
    "    print(f\"New Anomalous to Total ratio: {final_anomalous_count / len(oversampled_sessions):.2f}\")\n",
    "    return oversampled_sessions\n",
    "\n",
    "# --- Modified Session Creation Function ---\n",
    "def create_sessions_with_segment_ids(log_data, tokenizer, window, label_file=None):\n",
    "    \"\"\"\n",
    "    Process log data into sessions with input IDs and segment IDs.\n",
    "    Each session is defined by a unique block ID. For debugging purposes,\n",
    "    the session dictionary now includes the block ID.\n",
    "    \"\"\"\n",
    "    if window != 'session':\n",
    "        raise ValueError(\"Currently only window='session' is supported for the HDFS dataset.\")\n",
    "\n",
    "    # Group logs by block ID.\n",
    "    session_dict = {}\n",
    "    for line in tqdm(log_data, desc=\"Grouping logs by session\"):\n",
    "        tokens = line.split()\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            timestamp_str = \" \".join(tokens[:2])\n",
    "            timestamp = datetime.strptime(timestamp_str, '%y%m%d %H%M%S').timestamp()\n",
    "        except Exception as ex:\n",
    "            print(f\"Timestamp parsing error: {ex} in line: {line}\")\n",
    "            continue\n",
    "        \n",
    "        blk_ids = list(set(re.findall(r'(blk_-?\\d+)', line)))\n",
    "        if len(blk_ids) != 1:\n",
    "            continue\n",
    "        blk_id = blk_ids[0]\n",
    "        if clean_log == True:\n",
    "            cleaned_line = clean(line)\n",
    "        else:\n",
    "            cleaned_line = line        \n",
    "\n",
    "        if blk_id not in session_dict:\n",
    "            session_dict[blk_id] = []\n",
    "        session_dict[blk_id].append((timestamp, cleaned_line))\n",
    "    \n",
    "    # Load label mapping if provided.\n",
    "    label_mapping = {}\n",
    "    if label_file:\n",
    "        label_df = pd.read_csv(label_file, engine='c', na_filter=False)\n",
    "        label_df = label_df.set_index(\"BlockId\")\n",
    "        label_mapping = label_df[\"Label\"].to_dict()\n",
    "    \n",
    "    sessions = []\n",
    "    for blk_id, events in tqdm(session_dict.items(), desc=\"Processing sessions\"):\n",
    "        events.sort(key=lambda x: x[0])\n",
    "        log_sequence = [msg for (ts, msg) in events]\n",
    "        if label_file:\n",
    "            session_label = 1 if label_mapping.get(blk_id, \"Normal\") == \"Anomaly\" else 0\n",
    "        else:\n",
    "            session_label = 0\n",
    "        input_ids, segment_ids = tokenize_and_construct_input(log_sequence, tokenizer,max_token_length)\n",
    "        sessions.append({\n",
    "            \"block_id\": blk_id,   \n",
    "            \"input_ids\": input_ids,\n",
    "            \"segment_ids\": segment_ids,\n",
    "            \"session_label\": session_label\n",
    "        })\n",
    "    return sessions\n",
    "\n",
    "# --- Modified Main Loading Function ---\n",
    "def load_HDFS_with_gpt4_bpe_stratified(\n",
    "    log_file, \n",
    "    train_ratio=0.7, \n",
    "    val_ratio=0.15, \n",
    "    test_ratio=0.15, \n",
    "    window='session', \n",
    "    label_file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Load logs, create sessions, and perform a stratified train/val/test split based on session labels.\n",
    "    Uses a subset of log events (e.g., first 10,000) for quick testing.\n",
    "    \"\"\"\n",
    "    print(\"Loading logs from:\", log_file)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Read logs and create sessions.\n",
    "    with open(log_file, mode=\"r\", encoding='utf8') as f:\n",
    "       logs = [x.strip() for x in tqdm(f, desc=\"Reading Logs\")]\n",
    "    #logs = logs[:100000]  # For testing purposes\n",
    "    print(f\"Loaded {len(logs)} logs in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    # Load GPT-4 tokenizer.\n",
    "    tokenizer = load_gpt4_tokenizer()\n",
    "\n",
    "    # Create sessions from logs.\n",
    "    all_sessions = create_sessions_with_segment_ids(logs, tokenizer, window, label_file=label_file)\n",
    "\n",
    "    #calculate the number of length that fit XX% of the sessions \n",
    "    token_lengths = [len(session[\"input_ids\"]) for session in all_sessions]\n",
    "    max_len_90 = int(np.percentile(token_lengths, 90))\n",
    "    max_len_95 = int(np.percentile(token_lengths, 95))\n",
    "    max_token_length = max(token_lengths)\n",
    "    print(f\"Max tokens in 90% of sessions: {max_len_90}\")\n",
    "    print(f\"Max tokens in 95% of sessions: {max_len_95}\")\n",
    "    print(f\"Max tokens in 100% of sessions: {max_token_length}\") \n",
    "\n",
    "    \n",
    "    # Debug: Print first 10 session block IDs and labels.\n",
    "    # print(\"First 10 session block IDs and labels:\")\n",
    "    # for session in all_sessions[:10]:\n",
    "    #     print(f\"Block ID: {session['block_id']}  => Label: {session['session_label']}\")\n",
    "\n",
    "    # Check overlap between session block IDs and label file block IDs.\n",
    "    if label_file:\n",
    "        label_df = pd.read_csv(label_file, engine='c', na_filter=False)\n",
    "        label_df = label_df.set_index(\"BlockId\")\n",
    "        label_mapping = label_df[\"Label\"].to_dict()\n",
    "        session_block_ids = {session[\"block_id\"] for session in all_sessions}\n",
    "        label_block_ids = set(label_mapping.keys())\n",
    "        common_ids = session_block_ids.intersection(label_block_ids)\n",
    "        print(\"Total unique session block IDs:\", len(session_block_ids))\n",
    "        print(\"Total block IDs in label file:\", len(label_block_ids))\n",
    "        print(\"Number of block IDs common to both:\", len(common_ids))\n",
    "\n",
    "    # Stratified train/val/test split.\n",
    "    session_labels = [s[\"session_label\"] for s in all_sessions]\n",
    "    # First split: train and temp (val+test)\n",
    "    train_sessions, temp_sessions, train_labels, temp_labels = train_test_split(\n",
    "        all_sessions,\n",
    "        session_labels,\n",
    "        test_size=(1 - train_ratio),\n",
    "        stratify=session_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Compute val and test ratios relative to temp\n",
    "    val_relative = val_ratio / (val_ratio + test_ratio)\n",
    "    temp_labels = [s[\"session_label\"] for s in temp_sessions]\n",
    "    val_sessions, test_sessions, val_labels, test_labels = train_test_split(\n",
    "        temp_sessions,\n",
    "        temp_labels,\n",
    "        test_size=(1 - val_relative),\n",
    "        stratify=temp_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"Number of sessions: {len(all_sessions)}. Train: {len(train_sessions)}, Val: {len(val_sessions)}, Test: {len(test_sessions)}\")\n",
    "\n",
    "    # Print class distribution for each split\n",
    "    def print_split_stats(name, sessions):\n",
    "        normal = sum(s['session_label'] == 0 for s in sessions)\n",
    "        anomalous = sum(s['session_label'] == 1 for s in sessions)\n",
    "        ratio = anomalous / (normal + anomalous)\n",
    "        print(f\"{name} => Normal: {normal} | Anomalous: {anomalous}\")\n",
    "        print(f\"Anomalous to Total samples ratio in {name}: {ratio:.4f}\")\n",
    "\n",
    "    print_split_stats(\"Train set\", train_sessions)\n",
    "    print_split_stats(\"Validation set\", val_sessions)\n",
    "    print_split_stats(\"Test set\", test_sessions)\n",
    "\n",
    "    # Balance training data with oversampling.\n",
    "    print(\"Balancing training data with oversampling...\")\n",
    "    train_sessions = oversample_with_ratio_control(train_sessions, beta)\n",
    "    print(f\"Balanced training data: {len(train_sessions)} samples\")\n",
    "    print(f\"Total processing time: {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    return train_sessions, val_sessions, test_sessions, tokenizer\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    log_file = \"./logs/HDFS.log\"\n",
    "    label_file = \"./logs/anomaly_label_HDFS.csv\"\n",
    "    window = 'session'\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    test_ratio = 0.15\n",
    "    beta = 0.4\n",
    "    clean_log = False\n",
    "    set_seed(42)  # Set random seed for reproducibility\n",
    "\n",
    "    training_sessions, validation_sessions, test_sessions, tokenizer = load_HDFS_with_gpt4_bpe_stratified(\n",
    "        log_file, \n",
    "        train_ratio=train_ratio, \n",
    "        val_ratio=val_ratio,\n",
    "        test_ratio=test_ratio,\n",
    "        window=window,\n",
    "        label_file=label_file\n",
    "    )\n",
    "\n",
    "    # Validate sessions by printing a few tokenized sessions.\n",
    "    def validate_sessions(sessions, tokenizer, window):\n",
    "        print(\"=========== Start of Validate Sessions =====================\")\n",
    "        print(f\"Number of sessions: {len(sessions)}\")\n",
    "        for idx, session in enumerate(sessions[:5]):\n",
    "            print(f\"\\nSession {idx + 1}:\")\n",
    "            print(f\"Block ID: {session['block_id']}\")\n",
    "            print(f\"Input IDs: {session['input_ids']}\")\n",
    "            print(f\"Segment IDs: {session['segment_ids']}\")\n",
    "            print(f\"Session Label: {session['session_label']}\")\n",
    "            decoded_log = tokenizer.decode(session[\"input_ids\"])\n",
    "            print(f\"Decoded Log: {decoded_log}\")\n",
    "    \n",
    "    validate_sessions(training_sessions, tokenizer, window)\n",
    "\n",
    "    # calculate the average token and max tokens in the sessions\n",
    "\n",
    "    token_lengths = [len(session[\"input_ids\"]) for session in training_sessions]\n",
    "    max_token_length = max(token_lengths)\n",
    "    print(f\"Average tokens per session: {sum(token_lengths) / len(token_lengths):.2f}\")\n",
    "    print(f\"Max tokens in a session: {max_token_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# implement the embedding layers \n",
    "##############################################################################\n",
    "\n",
    "cl100k_vocab_size = 100264 # GPT4 BPE\n",
    "#segment_ids_size = 20  # for BGL  segment_vocab_size =  segment_ids_size = 20\n",
    "embedding_dimension = 128\n",
    "length_95_percentile = 1531 # for HDFS datase\n",
    "max_token_length = max_token_length  \n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size=cl100k_vocab_size, max_seq_len=max_token_length, segment_vocab_size=100, embedding_dim=128):   \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)          # GPT-2 vocab size\n",
    "        self.segment_embedding = nn.Embedding(segment_vocab_size, embedding_dim)  # For segment IDs\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)     # For position IDs\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, position_ids=None):\n",
    "        # Automatically generate position_ids if not provided\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0).repeat(input_ids.size(0), 1)\n",
    "        \n",
    "        E_token = self.token_embedding(input_ids)         # (batch_size, seq_len, embedding_dim)\n",
    "        E_segment = self.segment_embedding(segment_ids)   # (batch_size, seq_len, embedding_dim)\n",
    "        E_position = self.position_embedding(position_ids) # (batch_size, seq_len, embedding_dim)\n",
    "        return E_token + E_segment + E_position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Construct linear self-attention encoder via linformer\n",
    "# Final AllLinLog model\n",
    "###################################################################\n",
    "from linformer import Linformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "max_token_length = max_token_length  # Use the max token length from your dataset\n",
    "\n",
    "# Updated Transformer Encoder Layer using Linformer\n",
    "class LinformerEncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim=128, num_heads=2, ff_hidden_dim=128, seq_len=max_token_length, k=128, dropout=0.1):\n",
    "        \"\"\"\n",
    "        A single Linformer encoder layer.\n",
    "\n",
    "        Args:\n",
    "        - embedding_dim: Input embedding dimension.\n",
    "        - num_heads: Number of attention heads.\n",
    "        - ff_hidden_dim: Hidden dimension of the feed-forward network.\n",
    "        - k: Low-rank approximation factor for attention.\n",
    "        - dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(LinformerEncoderLayer, self).__init__()\n",
    "        self.self_attention = Linformer(\n",
    "            dim=embedding_dim,\n",
    "            seq_len=int(seq_len),  # Sequence length from your max_token_length\n",
    "            depth=1,  # Single depth for this layer\n",
    "            heads=num_heads,\n",
    "            k=k,  # Low-rank approximation\n",
    "            one_kv_head=True,\n",
    "            share_kv=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embedding_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        attention_output = self.self_attention(x)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "\n",
    "        # Feed-forward network with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "# Linformer-based Transformer Encoder\n",
    "class LinformerTransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers=1, embedding_dim=128, seq_len=max_token_length, num_heads=2, ff_hidden_dim=128, k=128, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Multi-layer Linformer-based Transformer encoder.\n",
    "\n",
    "        Args:\n",
    "        - num_layers: Number of Linformer encoder layers.\n",
    "        - embedding_dim: Input embedding dimension.\n",
    "        - num_heads: Number of attention heads.\n",
    "        - ff_hidden_dim: Hidden dimension of the feed-forward network.\n",
    "        - k: Low-rank approximation factor for attention.\n",
    "        - dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(LinformerTransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            LinformerEncoderLayer(embedding_dim, num_heads, ff_hidden_dim, seq_len, k, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Define the AllLinLog model using the Linformer encoder\n",
    "class AllLinLog(nn.Module):\n",
    "    def __init__(self, vocab_size=cl100k_vocab_size, max_seq_len=max_token_length, segment_vocab_size=20, embedding_dim=128, \n",
    "                 num_layers=1, num_heads=2, ff_hidden_dim=128, k=128, num_classes=2, dropout=0.1, max_segment_lengths=100):\n",
    "        super(AllLinLog, self).__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(vocab_size, max_seq_len, segment_vocab_size=max_segment_lengths, embedding_dim=embedding_dim)\n",
    "        self.encoder = LinformerTransformerEncoder(num_layers, embedding_dim, max_seq_len, num_heads, ff_hidden_dim, k, dropout)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)  # Final classification layer\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, position_ids, attention_mask=None):\n",
    "        embeddings = self.embedding_layer(input_ids, segment_ids, position_ids)  # (batch_size, seq_len, embedding_dim)\n",
    "        encoder_output = self.encoder(embeddings)  # (batch_size, seq_len, embedding_dim)\n",
    "        pooled_output = torch.mean(encoder_output, dim=1)  # Average pooling for session-level representation\n",
    "        logits = self.fc(pooled_output)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max segment length set to: 298\n",
      "Random seed set to 42\n",
      "running with k = 32\n",
      "Total Parameters: 15501506\n",
      "Model Size: 59.13 MB\n",
      "Input IDs: torch.Size([8, 1058]), Segment IDs: torch.Size([8, 1058])\n",
      "Attention Masks: torch.Size([8, 1058]), Session Labels: torch.Size([8])\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Classification Report:\n",
      "             precision   recall f1-score  support\n",
      "Normal         0.99988  0.99927  0.99958  83734.0\n",
      "Anomalous      0.97632  0.99604  0.98608   2525.0\n",
      "accuracy       0.99918  0.99918  0.99918      NaN\n",
      "macro avg      0.98810  0.99766  0.99283  86259.0\n",
      "weighted avg   0.99919  0.99918  0.99918  86259.0\n",
      "Train Loss: 0.01662, Train Accuracy: 0.99478\n",
      "Val Loss: 0.01058, Val Accuracy: 0.99918\n",
      "Anomalous Precision: 0.97632, Anomalous Recall: 0.99604, Anomalous F1 Score: 0.98608\n",
      "Training time for one epoch: 19.27 minutes\n",
      "The Memory initial allocated beform training: 313.02 MB\n",
      "Memory Used for training in this epoch: 177.68 MB\n",
      "\n",
      "Learning Rate: 5.0e-04\n",
      "New best model saved as : best_model_HDFS/best_model_HDFS20250825_195805.pth \n",
      "\n",
      "Average Training Time per Epoch: 19.272077445189158 min\n",
      "Peak GPU Memoryy Allocated: 1800.84 MB\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating best model on test set...\n",
      "Loaded best model from best_model_HDFS/best_model_HDFS20250825_195805.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Classification Report:\n",
      "             precision   recall f1-score  support\n",
      "Normal         0.99984  0.99928  0.99956  83734.0\n",
      "Anomalous      0.97668  0.99485  0.98568   2526.0\n",
      "accuracy       0.99915  0.99915  0.99915      NaN\n",
      "macro avg      0.98826  0.99707  0.99262  86260.0\n",
      "weighted avg   0.99917  0.99915  0.99916  86260.0\n",
      "\n",
      " Unbiased Test Classification Report:\n",
      "             precision   recall f1-score  support\n",
      "Normal         0.99984  0.99928  0.99956  83734.0\n",
      "Anomalous      0.97668  0.99485  0.98568   2526.0\n",
      "accuracy       0.99915  0.99915  0.99915      NaN\n",
      "macro avg      0.98826  0.99707  0.99262  86260.0\n",
      "weighted avg   0.99917  0.99915  0.99916  86260.0\n",
      "Loaded best model from best_model_HDFS/best_model_HDFS20250825_195805.pth\n",
      "Average Inference Time: 0.70 ms\n",
      "Memory Used During Inference: 0.00 MB\n",
      "Peak GPU Memory Allocated: 1800.84 MB\n",
      "Latest results file: results_HDFS/results_HDFS20250825_195805.txt\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Create the dataloaders \n",
    "# Training and Evaluation Pipeline\n",
    "# Output is placed into /results_HDFS\n",
    "# The best model is saved to /best_model_HDFS\n",
    "###############################################################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score as f1_score_sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "from linformer import Linformer\n",
    "import psutil\n",
    "import random\n",
    "\n",
    "def generate_classification_report(all_labels, all_preds):\n",
    "    # Generate the classification report as a dictionary\n",
    "    target_names = [\"Normal\", \"Anomalous\"]  # Adjust based on your dataset's classes\n",
    "    report_dict = classification_report(\n",
    "        all_labels, \n",
    "        all_preds, \n",
    "        target_names=target_names, \n",
    "        output_dict=True,\n",
    "        zero_division=0  # avoid warning for undefined metrics\n",
    "        )\n",
    "\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "    # Format numeric columns to desired precision\n",
    "    report_df['precision'] = report_df['precision'].apply(lambda x: f\"{x:.5f}\")\n",
    "    report_df['recall'] = report_df['recall'].apply(lambda x: f\"{x:.5f}\")\n",
    "    report_df['f1-score'] = report_df['f1-score'].apply(lambda x: f\"{x:.5f}\")\n",
    "    report_df['support'] = report_df['support'].apply(lambda x: int(x) if not pd.isna(x) else \"\")  # Format support as integer\n",
    "\n",
    "    # Remove support for the accuracy row\n",
    "    report_df.loc['accuracy', 'support'] = np.nan  # Use NaN for missing numeric values\n",
    "\n",
    "    print(\"Formatted Classification Report:\")\n",
    "    print(report_df)\n",
    "\n",
    "    # Optionally, save the report to a CSV or text file\n",
    "    report_df.to_csv('classification_report.csv', index=True)\n",
    "    return report_df\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, scheduler, criterion, num_epochs=10, patience=20, device='cpu'):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.criterion = criterion\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        self.total_time = 0\n",
    "        self.timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        # Create necessary folders\n",
    "        os.makedirs('results_HDFS', exist_ok=True)\n",
    "        os.makedirs('best_model_HDFS', exist_ok=True)\n",
    "\n",
    "    def train(self):\n",
    "        best_val_loss = float('inf')\n",
    "        best_val_f1 = 0.0\n",
    "        patience_counter = 0   \n",
    "        results_file = os.path.join('results_HDFS', f'results_HDFS{self.timestamp}.txt')\n",
    "        best_model_file = os.path.join('best_model_HDFS', f'best_model_HDFS{self.timestamp}.pth')\n",
    "        total_params, size_mb = compute_model_size(self.model)\n",
    "        total_time_minutes = 0 \n",
    "        memory_initial = 0\n",
    "\n",
    "        with open(results_file, 'w') as f:\n",
    "            f.write(\"Experiment Parameters:\\n\")\n",
    "            f.write(f\"log_file: {log_file}\\n\")\n",
    "            #f.write(f\"windows_size: {windows_size}\\n\") # not used for HDFS\n",
    "            #f.write(f\"step_size: {step_size}\\n\") # not used for HDFS\n",
    "            f.write(f\"train_ratio: {train_ratio}\\n\")\n",
    "            f.write(f\"beta: {beta}\\n\")\n",
    "            f.write(f\"batch_size: {batch_size}\\n\")\n",
    "            f.write(f\"dropout: {dropout}\\n\")\n",
    "            f.write(f\"num_layers: {num_layers}\\n\")\n",
    "            f.write(f\"num_heads: {num_heads}\\n\")\n",
    "            f.write(f\"k: {k}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\"Total Parameters: {total_params}\\n\")\n",
    "            f.write(f\"Model Size: {size_mb:.2f} MB\\n\")\n",
    "            memory_initial = measure_memory()\n",
    "            epoch_run = 0\n",
    "            for epoch in range(self.num_epochs):\n",
    "                start_time = time.time()\n",
    "                memory_before = measure_memory()\n",
    "                epoch_run +=1\n",
    "                print(f\"Epoch {epoch + 1}/{self.num_epochs}\")\n",
    "                print(f\"Epoch {epoch + 1}/{self.num_epochs}\", file=f)\n",
    "\n",
    "                # Training Phase\n",
    "                train_loss, train_acc = self._train_one_epoch()\n",
    "                epoch_time_minutes = (time.time() - start_time) / 60\n",
    "                total_time_minutes += epoch_time_minutes\n",
    "\n",
    "                memory_after = measure_memory()\n",
    "                if memory_before == memory_after:\n",
    "                    memory_before = memory_initial\n",
    "\n",
    "                # Validation Phase\n",
    "                val_loss, val_acc, Anomalous_val_precision, Anomalous_val_recall, Anomalous_val_f1, report_df = self._validate()\n",
    "\n",
    "                # Log and Save Metrics\n",
    "                self.train_losses.append(train_loss)\n",
    "                self.train_accuracies.append(train_acc)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accuracies.append(val_acc)\n",
    "            \n",
    "                # Log to console and file\n",
    "                log = (\n",
    "                    f\"Train Loss: {float(train_loss):.5f}, Train Accuracy: {float(train_acc):.5f}\\n\"\n",
    "                    f\"Val Loss: {float(val_loss):.5f}, Val Accuracy: {float(val_acc):.5f}\\n\"\n",
    "                    f\"Anomalous Precision: {float(Anomalous_val_precision):.5f}, \"\n",
    "                    f\"Anomalous Recall: {float(Anomalous_val_recall):.5f}, \"\n",
    "                    f\"Anomalous F1 Score: {float(Anomalous_val_f1):.5f}\\n\"\n",
    "                    f\"Training time for one epoch: {float(epoch_time_minutes):.2f} minutes\\n\"\n",
    "                    f\"The Memory initial allocated beform training: {memory_initial:.2f} MB\\n\"\n",
    "                    f\"Memory Used for training in this epoch: {memory_after - memory_before:.2f} MB\\n\"\n",
    "                )\n",
    "\n",
    "                print(log)\n",
    "                print(log, file=f)\n",
    "\n",
    "                # Log the full classification report to the results file\n",
    "                print(\"\\nClassification Report:\\n\", file=f)\n",
    "                print(report_df.to_string(), file=f)\n",
    "                print(\"------------ END of Epoch --------------\\n\", file=f)\n",
    "\n",
    "                # Scheduler Step\n",
    "                self.scheduler.step(val_loss)\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                print(f\"Learning Rate: {current_lr:.1e}\")\n",
    "                print(f\"Learning Rate: {current_lr:.1e}\", file=f)\n",
    "\n",
    "                # Early Stopping Logic , can based on val_loss or Anmalous_val_f1\n",
    "                if (val_loss < best_val_loss) or (Anomalous_val_f1 > best_val_f1):\n",
    "                    best_val_loss = min(val_loss, best_val_loss)\n",
    "                    best_val_f1 = max(Anomalous_val_f1, best_val_f1)\n",
    "                    patience_counter = 0\n",
    "                    torch.save(self.model.state_dict(), best_model_file)\n",
    "                    print(f\"New best model saved as : { best_model_file } \\n\")\n",
    "                    print(f\"New best model saved as : { best_model_file } \\n\", file=f)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.patience:\n",
    "                        print(\"Early stopping triggered.\\n\")\n",
    "                        print(\"Early stopping triggered.\\n\", file=f)\n",
    "                        break\n",
    "            \n",
    "            # average training time per epoch \n",
    "            avg_time_minutes = total_time_minutes / epoch_run\n",
    "            print(f\"Average Training Time per Epoch: {avg_time_minutes} min\")\n",
    "            print(f\"Average Training Time per Epoch: {avg_time_minutes} min\", file=f)\n",
    "\n",
    "            # peak GPU memory allocated\n",
    "            if torch.cuda.is_available():\n",
    "                print(f\"Peak GPU Memoryy Allocated: {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\")\n",
    "                print(f\"Peak GPU Memoryy Allocated: {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\", file=f)\n",
    "            # Plot Training History\n",
    "            self._plot_training_history()\n",
    "\n",
    "    def _train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loop = tqdm(self.train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for batch in loop:\n",
    "            input_ids, segment_ids, attention_masks, labels = [b.to(self.device) for b in batch]\n",
    "            # Debug: Check that all labels are in {0, 1}\n",
    "            if not torch.all((labels == 0) | (labels == 1)):\n",
    "                print(\"Invalid label found in batch:\", labels)\n",
    "                raise ValueError(\"Target labels must be 0 or 1 for CrossEntropyLoss.\")\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            logits = self.model(input_ids, segment_ids, attention_masks)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            #----------addind for reduce overfitting ---------------\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            #--------------------------------------------------------\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), accuracy=correct / total)\n",
    "\n",
    "        return total_loss / len(self.train_loader), correct / total\n",
    "\n",
    "    def _validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        loop = tqdm(self.val_loader, desc=\"Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch in loop:\n",
    "                input_ids, segment_ids, attention_masks, labels = [b.to(self.device) for b in batch]\n",
    "                logits = self.model(input_ids, segment_ids, attention_masks)\n",
    "                loss = self.criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                #--for debug, show the partial f1 score---\n",
    "                # Compute partial F1 with current predictions\n",
    "                partial_f1 = f1_score_sklearn(all_labels, all_preds, pos_label=1,zero_division=0)\n",
    "                loop.set_postfix(partial_f1=f\"{partial_f1:.5f}\", loss=loss.item())\n",
    "                \n",
    "        # Generate formatted classification report\n",
    "        report_df = generate_classification_report(all_labels, all_preds)\n",
    "        # Extract specific metrics\n",
    "        if \"Anomalous\" in report_df.index:\n",
    "            precision = float(report_df.loc[\"Anomalous\", \"precision\"])\n",
    "            recall = float(report_df.loc [\"Anomalous\", \"recall\"])\n",
    "            f1_score = float(report_df.loc[\"Anomalous\", \"f1-score\"])\n",
    "        else:\n",
    "            precision, recall, f1_score = 0.0, 0.0, 0.0\n",
    "\n",
    "        \n",
    "\n",
    "        # Overall accuracy\n",
    "        accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "\n",
    "        return total_loss / len(self.val_loader), accuracy, precision, recall, f1_score, report_df\n",
    "\n",
    "    def measure_inference_time(self, input_sample, num_runs=100):\n",
    "        \"\"\"measure inference time and memory usage\"\"\"\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        input_sample = [tensor.to(self.device) for tensor in input_sample]\n",
    "        with torch.no_grad():\n",
    "            # measure memory before inference\n",
    "            memory_before = measure_memory()\n",
    "\n",
    "            for _ in range(10):    # warm up befor actual benchmarking \n",
    "                _ = self.model(*input_sample)\n",
    "            start_time = time.time()\n",
    "            for _ in range(num_runs):\n",
    "                _ = self.model(*input_sample)\n",
    "            total_time = time.time() - start_time\n",
    "            # measure memory after inference\n",
    "            memory_after = measure_memory()\n",
    "\n",
    "        avg_inference_time = (total_time / num_runs) * 1000  # Convert to milliseconds\n",
    "        print(f\"Average Inference Time: {avg_inference_time:.2f} ms\")\n",
    "        mem_during_inference = memory_after - memory_before\n",
    "        print(f\"Memory Used During Inference: {mem_during_inference:.2f} MB\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            GPU_max_mem_during_inference = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "            print(f\"Peak GPU Memory Allocated: {GPU_max_mem_during_inference:.2f} MB\")\n",
    "        return avg_inference_time, mem_during_inference, GPU_max_mem_during_inference\n",
    "\n",
    "    def _plot_training_history(self):\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        training_graph = os.path.join('results', f'training_history_{self.timestamp}.png')\n",
    "\n",
    "        # Plot Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.train_losses, label='Train Loss')\n",
    "        plt.plot(epochs, self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, self.train_accuracies, label='Train Accuracy')\n",
    "        plt.plot(epochs, self.val_accuracies, label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(training_graph)\n",
    "        plt.show()\n",
    "\n",
    "# collate_fn for dataloader to handle variable length of session of logs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for training_sessions.\n",
    "\n",
    "    Args:\n",
    "    - batch: List of dictionaries with keys \"input_ids\", \"segment_ids\", \"session_label\".\n",
    "\n",
    "    Returns:\n",
    "    - input_ids: Padded tensor of shape (batch_size, max_seq_length).\n",
    "    - segment_ids: Padded tensor of shape (batch_size, max_seq_length).\n",
    "    - attention_masks: Tensor of shape (batch_size, max_seq_length).\n",
    "    - session_labels: Tensor of shape (batch_size).\n",
    "    \"\"\"\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"], dtype=torch.long) for item in batch]\n",
    "    segment_ids = [torch.tensor(item[\"segment_ids\"], dtype=torch.long) for item in batch]\n",
    "    session_labels = torch.tensor([item[\"session_label\"] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    padded_segment_ids = pad_sequence(segment_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Clamp padded sequences to max_token_length after padding\n",
    "    padded_input_ids = padded_input_ids[:, :max_token_length]\n",
    "    padded_segment_ids = padded_segment_ids[:, :max_token_length]\n",
    "\n",
    "    \n",
    "    # Create attention masks: 1 for non-padding, 0 for padding\n",
    "    attention_masks = (padded_input_ids != 0).long()\n",
    "\n",
    "    \n",
    "\n",
    "    return padded_input_ids, padded_segment_ids, attention_masks, session_labels\n",
    "\n",
    "def compute_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_mb = total_params * 4 /(1024**2) # assuming 32 bits floating points\n",
    "    return total_params, size_mb\n",
    "\n",
    "def measure_memory():\n",
    "    \"\"\"Measure GPU and CPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / (1024 ** 2)  # Convert to MB\n",
    "    else:\n",
    "        return psutil.Process().memory_info().rss / (1024 ** 2)  # RAM in MB\n",
    "\n",
    "\n",
    "def get_latest_result_file(results_dir=\"results_HDFS\"):\n",
    "    \"\"\"Get the most recent output file in the results folder.\"\"\"\n",
    "    list_of_files = glob.glob(os.path.join(results_dir, \"results_*.txt\"))  # Get all result files\n",
    "    if not list_of_files:\n",
    "        print(\"No result files found.\")\n",
    "        return None\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)  # Get the newest file\n",
    "    print(f\"Latest results file: {latest_file}\")\n",
    "    return latest_file\n",
    "\n",
    "def load_best_model(model_path, model_class, k, device):\n",
    "    \"\"\"\n",
    "    Load the best saved model from disk.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model.\n",
    "        model_class (torch.nn.Module): Model class to instantiate.\n",
    "        device (str): Device to load the model onto.\n",
    "    \n",
    "    Returns:\n",
    "        model (torch.nn.Module): Loaded model with trained weights.\n",
    "    \"\"\"\n",
    "    model = model_class(\n",
    "        vocab_size=cl100k_vocab_size,\n",
    "        embedding_dim=embedding_dim, max_seq_len=max_token_length, num_layers=num_layers, \n",
    "        num_heads=num_heads, k=k, ff_hidden_dim=ff_hidden_dim, num_classes=2,   \n",
    "        dropout=dropout, max_segment_lengths=max_segment_lengths\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    model.eval()\n",
    "    print(f\"Loaded best model from {model_path}\")\n",
    "    return model\n",
    "\n",
    "# Label Smoothing Loss Implementation\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        \"\"\"\n",
    "        smoothing: amount of smoothing (e.g., 0.1 means 10% smoothing)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(-1)\n",
    "        # Convert target to one-hot encoding with smoothing\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logits)\n",
    "            true_dist.fill_(self.smoothing / (num_classes - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        return torch.mean(torch.sum(-true_dist * F.log_softmax(logits, dim=-1), dim=-1))\n",
    "    \n",
    "# define the max_segment_id for HDFS dataset\n",
    "def compute_max_segment_id(sessions):\n",
    "    \"\"\"\n",
    "    Computes the maximum segment count across sessions.\n",
    "    \n",
    "    For each session, it assumes that the segment IDs indicate the log event index.\n",
    "    If a session has segment IDs like [0, 0, 0, 1, 1, 1, 2, 2, 2] (for 3 log events),\n",
    "    then the maximum segment ID is 2, and the count of log events is 2+1 = 3.\n",
    "    \"\"\"\n",
    "    max_seg = 0\n",
    "    for session in sessions:\n",
    "        if session[\"segment_ids\"]:\n",
    "            # Compute the maximum segment id in this session and add 1 to get the count.\n",
    "            seg_count = max(session[\"segment_ids\"]) + 1\n",
    "            if seg_count > max_seg:\n",
    "                max_seg = seg_count\n",
    "    return max_seg\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "\n",
    "\n",
    "# train and validate \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "batch_size = 8\n",
    "dropout = 0.4\n",
    "embedding_dim = 128\n",
    "ff_hidden_dim = 128\n",
    "num_layers = 1\n",
    "num_heads = 4\n",
    "num_epochs = 50 # 50 for HDFS evaluation\n",
    "k = 32\n",
    "max_token_length = max_token_length  # Use the max token length from your dataset\n",
    "seed = 42  # Set a seed for reproducibility\n",
    "\n",
    "train_max_segment = compute_max_segment_id(training_sessions)\n",
    "test_max_segment = compute_max_segment_id(test_sessions)\n",
    "max_segment_lengths = max(train_max_segment, test_max_segment)\n",
    "print(\"Max segment length set to:\", max_segment_lengths)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #for k in [256, 128, 64, 32, 16, 8, 4]: \n",
    "        set_seed(seed)\n",
    "        print(f\"running with k = {k}\")\n",
    "        \n",
    "        model = AllLinLog(\n",
    "            vocab_size=cl100k_vocab_size,\n",
    "            embedding_dim=embedding_dim, max_seq_len=max_token_length, num_layers=num_layers, \n",
    "            num_heads=num_heads, k=k, ff_hidden_dim=ff_hidden_dim, num_classes=2,   \n",
    "            dropout=dropout, max_segment_lengths=max_segment_lengths\n",
    "        ).to(device)\n",
    "\n",
    "        total_params, size_mb = compute_model_size(model)\n",
    "        print(f\"Total Parameters: {total_params}\")\n",
    "        print(f\"Model Size: {size_mb:.2f} MB\")\n",
    "\n",
    "        #optimizer = optim.Adam(model.parameters(), lr=5e-4)  # Initial learning rate from the paper\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2, eps=1e-8) # validataiton get improved after this change\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20)\n",
    "\n",
    "        # criterion = nn.CrossEntropyLoss()\n",
    "        class_weights = torch.tensor([1.0, 4.0]).to(device)  # Heavily penalize minority class misclassification\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        train_dataset = LogDataset(training_sessions)\n",
    "        val_dataset = LogDataset(validation_sessions)  # Use validation split\n",
    "        test_dataset = LogDataset(test_sessions)      # Use test split\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            collate_fn=collate_fn, \n",
    "            shuffle=True, \n",
    "            pin_memory=True  # Optimize data transfer to GPU\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            collate_fn=collate_fn, \n",
    "            pin_memory=True  # Optimize data transfer to GPU\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            collate_fn=collate_fn, \n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "\n",
    "        # Verify data\n",
    "        for batch in train_loader:\n",
    "            input_ids, segment_ids, attention_masks, session_labels = batch\n",
    "            print(f\"Input IDs: {input_ids.shape}, Segment IDs: {segment_ids.shape}\")\n",
    "            print(f\"Attention Masks: {attention_masks.shape}, Session Labels: {session_labels.shape}\")\n",
    "            break\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            criterion=criterion,\n",
    "            num_epochs=num_epochs,  # Adjust if needed\n",
    "            patience=30,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # --- Post-training: Evaluate on test set ---\n",
    "        print(\"\\nEvaluating best model on test set...\")\n",
    "        model_path = os.path.join('best_model_HDFS', f'best_model_HDFS{trainer.timestamp}.pth')\n",
    "        model = load_best_model(model_path, AllLinLog, k, device)\n",
    "        test_trainer = Trainer(model, None, None, None, None, None, num_epochs=1, device=device)\n",
    "\n",
    "        model.eval()\n",
    "        all_test_preds = []\n",
    "        all_test_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "                input_ids, segment_ids, attention_masks, labels = [b.to(device) for b in batch]\n",
    "                logits = model(input_ids, segment_ids, attention_masks)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                all_test_preds.extend(preds.cpu().numpy())\n",
    "                all_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        test_report_df = generate_classification_report(all_test_labels, all_test_preds)\n",
    "        print(\"\\n Unbiased Test Classification Report:\")\n",
    "        print(test_report_df)\n",
    "\n",
    "        # Save test report to results_HDFS/test_result_HDFS_{timestamp}.csv and .txt\n",
    "        os.makedirs('results_HDFS', exist_ok=True)\n",
    "        test_csv_path = os.path.join('results_HDFS', f'test_result_HDFS_{trainer.timestamp}.csv')\n",
    "        test_txt_path = os.path.join('results_HDFS', f'test_result_HDFS_{trainer.timestamp}.txt')\n",
    "        test_report_df.to_csv(test_csv_path, index=True)\n",
    "        with open(test_txt_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(str(test_report_df))\n",
    "\n",
    "        # --- Inference time measurement (unchanged) ---\n",
    "        # Get a sample from the validation dataset\n",
    "        sample = val_dataset[0]  # Extract the first sample (or randomly pick one)\n",
    "        input_sample = (\n",
    "        torch.tensor(sample[\"input_ids\"], dtype=torch.long).unsqueeze(0).to(device),   # Ensure input_ids is LongTensor\n",
    "        torch.tensor(sample[\"segment_ids\"], dtype=torch.long).unsqueeze(0).to(device), # Ensure segment_ids is LongTensor\n",
    "        torch.arange(len(sample[\"input_ids\"]), dtype=torch.long).unsqueeze(0).to(device),  # Corrected position_ids\n",
    "        torch.ones(1, len(sample[\"input_ids\"]), dtype=torch.float).to(device)  # attention_mask remains FloatTensor\n",
    "        )\n",
    "\n",
    "        # load the best model for averaging inference time \n",
    "        model_path = os.path.join('best_model_HDFS', f'best_model_HDFS{trainer.timestamp}.pth')\n",
    "        model = load_best_model(model_path, AllLinLog, k, device)\n",
    "        trainer = Trainer(model, None, None, None, None, None, num_epochs=10, device=device)\n",
    "\n",
    "        # Measure inference time using a real validation dataset sample and log it \n",
    "        avg_inference_time, mem_during_inference, GPU_max_mem_during_inference  = trainer.measure_inference_time(input_sample, num_runs=100)\n",
    "        result_file = get_latest_result_file()\n",
    "        with open(result_file, 'a') as f:\n",
    "            print(f\"Average Inference Time: {avg_inference_time:.2f} ms \\n\", file=f)\n",
    "            print(f\"Memory Used During Inference: {mem_during_inference:.2f} MB\\n\", file=f)\n",
    "            print(f\"GPU max memory allocated during inference: {GPU_max_mem_during_inference:.2f} MB\\n\", file=f)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
