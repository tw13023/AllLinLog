{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook trains and evaluates AllLingLog on BGL dataset. \n",
    "## Requirements\n",
    "\n",
    "1. `torch==2.7.1+cu128`\n",
    "2. `numpy==2.3.1`\n",
    "3. `pandas==2.3.1`\n",
    "4. `scikit-learn==1.7.0`\n",
    "5. `tqdm==4.67.1`\n",
    "6. `tiktoken==0.9.0`\n",
    "7. `linformer==0.2.3`\n",
    "8. `psutil==7.0.0`\n",
    "9. `matplotlib==3.10.3`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n",
      "Loading logs from: ./logs/BGL.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Logs: 4747963it [00:01, 3505198.59it/s]\n",
      "Reading Logs: 4747963it [00:01, 3505198.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4747963 logs in 1.36 seconds.\n",
      "Loading cl100k_base (GPT-4) tokenizer...\n",
      "Creating sessions...\n",
      "Creating sessions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions: 100%|██████████| 474796/474796 [01:41<00:00, 4662.06it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens in 90% of sessions: 891\n",
      "Max tokens in 95% of sessions: 891\n",
      "Max tokens in 100% of sessions: 2549\n",
      "Number of sessions after sliding window: 474796\n",
      "Train sessions: 332357 | Validation sessions: 71219 | Test sessions: 71220\n",
      "\n",
      " The ratio of Normal to Animalous befor oversampling\n",
      "Train set => Normal: 305042 | Anomalous: 27315\n",
      "Anomalous ratio:  0.08\n",
      "Validation set => Normal: 65366 | Anomalous: 5853\n",
      "Anomalous ratio:  0.08\n",
      "Test set => Normal: 65366 | Anomalous: 5854\n",
      "Anomaloud ratio:  0.08 \n",
      "\n",
      "Balancing training data with oversampling...\n",
      "Number of sessions after sliding window: 474796\n",
      "Train sessions: 332357 | Validation sessions: 71219 | Test sessions: 71220\n",
      "\n",
      " The ratio of Normal to Animalous befor oversampling\n",
      "Train set => Normal: 305042 | Anomalous: 27315\n",
      "Anomalous ratio:  0.08\n",
      "Validation set => Normal: 65366 | Anomalous: 5853\n",
      "Anomalous ratio:  0.08\n",
      "Test set => Normal: 65366 | Anomalous: 5854\n",
      "Anomaloud ratio:  0.08 \n",
      "\n",
      "Balancing training data with oversampling...\n",
      "Original Minority Samples: 27315\n",
      "New Target Minority Size: 203361\n",
      "New Total Samples: 508403\n",
      "\n",
      " The ratio of Normal to Animalous after oversampling\n",
      "Train set => Normal: 305042 | Anomalous: 203361\n",
      "Train Anomalous ratio:  0.39999960661129064\n",
      "Validation set => Normal: 65366 | Anomalous: 5853\n",
      "Validation Anomalous ratio:  0.08218312528959969\n",
      "Test set => Normal: 65366 | Anomalous: 5854\n",
      "Test Anomalous ratio:  0.08219601235607975 \n",
      "\n",
      "Balanced training data: 508403 samples\n",
      "Total processing time: 106.26 seconds.\n",
      "Original Minority Samples: 27315\n",
      "New Target Minority Size: 203361\n",
      "New Total Samples: 508403\n",
      "\n",
      " The ratio of Normal to Animalous after oversampling\n",
      "Train set => Normal: 305042 | Anomalous: 203361\n",
      "Train Anomalous ratio:  0.39999960661129064\n",
      "Validation set => Normal: 65366 | Anomalous: 5853\n",
      "Validation Anomalous ratio:  0.08218312528959969\n",
      "Test set => Normal: 65366 | Anomalous: 5854\n",
      "Test Anomalous ratio:  0.08219601235607975 \n",
      "\n",
      "Balanced training data: 508403 samples\n",
      "Total processing time: 106.26 seconds.\n",
      "=========== Start of Validate Sessions =====================\n",
      "Number of sessions: 508403\n",
      "\n",
      "Session 1:\n",
      "Input IDs: [27, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1032, 35681, 806, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 26563, 22708, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1032, 35681, 806, 432, 1950, 80791, 31871, 24038, 6332, 13, 8929, 5547, 100257, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1114, 35681, 806, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 25747, 12112, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1114, 35681, 806, 432, 1950, 80791, 31871, 24038, 6332, 13, 8929, 1399, 100257, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2304, 35681, 1721, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 22889, 23776, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2304, 35681, 1721, 432, 1950, 80791, 31871, 24038, 6332, 13, 8929, 2618, 100257, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2839, 35681, 1721, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 26026, 5120, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2839, 35681, 1721, 432, 1950, 80791, 31871, 24038, 6332, 13, 10161, 2946, 100257, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2304, 35681, 806, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 24597, 20615, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2304, 35681, 806, 432, 1950, 80791, 31871, 24038, 6332, 13, 8929, 5495, 100257, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2839, 35681, 806, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 25476, 20077, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2839, 35681, 806, 432, 1950, 80791, 31871, 24038, 6332, 13, 10161, 2075, 100257, 7261, 15966, 25921, 24, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2589, 35681, 806, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2545, 13, 11030, 11256, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2589, 35681, 806, 432, 1950, 80791, 31871, 24038, 6332, 13, 10161, 5728, 100257, 7261, 15966, 25921, 24, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 868, 35681, 1721, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2545, 13, 21040, 23642, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 868, 35681, 1721, 432, 1950, 80791, 31871, 24038, 6332, 13, 10161, 3487, 100257, 7261, 15966, 25921, 24, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1114, 35681, 1721, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2545, 13, 27033, 20478, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1114, 35681, 1721, 432, 1950, 80791, 31871, 24038, 6332, 13, 8929, 2096, 100257, 7261, 15966, 25921, 24, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 806, 35681, 1721, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2545, 13, 24646, 24495, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 806, 35681, 1721, 432, 1950, 80791, 31871, 24038, 6332, 13, 10161, 3226, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "Session Label: 0\n",
      "Decoded Log: <1122580568 2005.07.28 R20-M0-N2-C:J13-U11 2005-07-28-12.56.08.846712 R20-M0-N2-C:J13-U11 RAS KERNEL INFO generating core.14461<|endoftext|>1122580568 2005.07.28 R20-M0-N2-C:J17-U11 2005-07-28-12.56.08.873215 R20-M0-N2-C:J17-U11 RAS KERNEL INFO generating core.14460<|endoftext|>1122580568 2005.07.28 R20-M0-N2-C:J05-U01 2005-07-28-12.56.08.899757 R20-M0-N2-C:J05-U01 RAS KERNEL INFO generating core.14447<|endoftext|>1122580568 2005.07.28 R20-M0-N2-C:J03-U01 2005-07-28-12.56.08.926110 R20-M0-N2-C:J03-U01 RAS KERNEL INFO generating core.14959<|endoftext|>1122580568 2005.07.28 R20-M0-N2-C:J05-U11 2005-07-28-12.56.08.952590 R20-M0-N2-C:J05-U11 RAS KERNEL INFO generating core.14463<|endoftext|>1122580568 2005.07.28 R20-M0-N2-C:J03-U11 2005-07-28-12.56.08.979394 R20-M0-N2-C:J03-U11 RAS KERNEL INFO generating core.14975<|endoftext|>1122580569 2005.07.28 R20-M0-N2-C:J07-U11 2005-07-28-12.56.09.006178 R20-M0-N2-C:J07-U11 RAS KERNEL INFO generating core.14974<|endoftext|>1122580569 2005.07.28 R20-M0-N2-C:J15-U01 2005-07-28-12.56.09.032569 R20-M0-N2-C:J15-U01 RAS KERNEL INFO generating core.14956<|endoftext|>1122580569 2005.07.28 R20-M0-N2-C:J17-U01 2005-07-28-12.56.09.059064 R20-M0-N2-C:J17-U01 RAS KERNEL INFO generating core.14444<|endoftext|>1122580569 2005.07.28 R20-M0-N2-C:J11-U01 2005-07-28-12.56.09.085638 R20-M0-N2-C:J11-U01 RAS KERNEL INFO generating core.14957<|endoftext|>\n",
      "Number of logs in this session: 10\n",
      "\n",
      "Session 2:\n",
      "Input IDs: [27, 5037, 22869, 21144, 22, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1806, 13, 18517, 12994, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 22, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1806, 13, 20936, 24491, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 22, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1806, 13, 19774, 22922, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 22, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1806, 13, 24344, 14205, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 22, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1806, 13, 16415, 22058, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 23, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1987, 13, 8190, 22904, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 23, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1987, 13, 11584, 23505, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 23, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1987, 13, 20153, 12652, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 23, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1987, 13, 22048, 19027, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 23, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1987, 13, 25388, 26223, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "Session Label: 1\n",
      "Decoded Log: <1118554777 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.37.424233 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554777 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.37.522663 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554777 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.37.667613 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554777 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.37.844246 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554777 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.37.960541 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554778 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.38.113833 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554778 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.38.214907 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554778 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.38.433209 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554778 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.38.546710 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554778 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.38.697834 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>\n",
      "Number of logs in this session: 10\n",
      "\n",
      "Session 3:\n",
      "Input IDs: [27, 8190, 17058, 10350, 23, 220, 1049, 20, 13, 806, 13, 1187, 432, 1958, 5364, 15, 11500, 23, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2166, 13, 23103, 22801, 432, 1958, 5364, 15, 11500, 23, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 18395, 435, 31174, 272, 3205, 25, 4703, 5403, 1984, 9436, 389, 356, 822, 3103, 7728, 311, 220, 10861, 13, 845, 13, 4161, 13, 8027, 25, 21505, 1806, 11, 6074, 706, 1027, 84450, 100257, 8190, 17058, 10350, 23, 220, 1049, 20, 13, 806, 13, 1187, 432, 1644, 5364, 15, 11500, 23, 22197, 25, 41, 972, 35681, 806, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2166, 13, 25110, 17763, 432, 1644, 5364, 15, 11500, 23, 22197, 25, 41, 972, 35681, 806, 432, 1950, 80791, 31871, 272, 3205, 25, 39517, 8450, 220, 868, 11, 2082, 28, 15, 11, 28053, 28, 15, 11, 2686, 28, 15, 87, 931, 4119, 69, 17, 100257, 8190, 17058, 10350, 23, 220, 1049, 20, 13, 806, 13, 1187, 432, 1958, 5364, 16, 11500, 23, 22197, 25, 41, 972, 35681, 806, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2166, 13, 21278, 18775, 432, 1958, 5364, 16, 11500, 23, 22197, 25, 41, 972, 35681, 806, 432, 1950, 80791, 31871, 272, 3205, 25, 39517, 8450, 220, 868, 11, 2082, 28, 15, 11, 28053, 28, 15, 11, 2686, 28, 15, 87, 931, 4119, 69, 17, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 806, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 24254, 22210, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 806, 432, 1950, 80791, 31871, 272, 3205, 25, 39517, 8450, 220, 868, 11, 2082, 28, 15, 11, 28053, 28, 15, 11, 2686, 28, 15, 87, 931, 4119, 69, 17, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 806, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 14057, 5245, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 806, 432, 1950, 18395, 435, 31174, 272, 3205, 25, 4703, 5403, 1984, 9436, 389, 356, 822, 3103, 7728, 311, 220, 10861, 13, 845, 13, 4161, 13, 8027, 25, 21757, 5833, 11, 6074, 706, 1027, 84450, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 17662, 10898, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 18395, 435, 31174, 272, 3205, 25, 4703, 5403, 1984, 9436, 389, 356, 822, 3103, 7728, 311, 220, 10861, 13, 845, 13, 4161, 13, 8027, 25, 21757, 6069, 11, 6074, 706, 1027, 84450, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 1313, 5364, 15, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 21312, 23215, 432, 1313, 5364, 15, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 80791, 31871, 272, 3205, 25, 39517, 8450, 220, 868, 11, 2082, 28, 15, 11, 28053, 28, 15, 11, 2686, 28, 15, 87, 931, 4119, 69, 20, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 1313, 5364, 15, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 20744, 15894, 432, 1313, 5364, 15, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 18395, 435, 31174, 272, 3205, 25, 4703, 5403, 1984, 9436, 389, 356, 822, 3103, 7728, 311, 220, 10861, 13, 845, 13, 4161, 13, 8027, 25, 20617, 5932, 11, 6074, 706, 1027, 84450, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 1419, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 23105, 24344, 432, 1419, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 80791, 31871, 272, 3205, 25, 39517, 8450, 220, 868, 11, 2082, 28, 15, 11, 28053, 28, 15, 11, 2686, 28, 15, 87, 931, 4119, 69, 17, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 1419, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 26366, 15574, 432, 1419, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 18395, 435, 31174, 272, 3205, 25, 4703, 5403, 1984, 9436, 389, 356, 822, 3103, 7728, 311, 220, 10861, 13, 845, 13, 4161, 13, 8027, 25, 21757, 2371, 11, 6074, 706, 1027, 84450, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "Session Label: 1\n",
      "Decoded Log: <1132841388 2005.11.24 R34-M0-N8-I:J18-U01 2005-11-24-06.09.48.627623 R34-M0-N8-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:47337, Link has been severed<|endoftext|>1132841388 2005.11.24 R33-M0-N8-I:J18-U11 2005-11-24-06.09.48.761416 R33-M0-N8-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2<|endoftext|>1132841388 2005.11.24 R34-M1-N8-I:J18-U11 2005-11-24-06.09.48.909367 R34-M1-N8-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2<|endoftext|>1132841389 2005.11.24 R30-M1-NC-I:J18-U11 2005-11-24-06.09.49.063564 R30-M1-NC-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2<|endoftext|>1132841389 2005.11.24 R30-M1-NC-I:J18-U11 2005-11-24-06.09.49.226180 R30-M1-NC-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:47284, Link has been severed<|endoftext|>1132841389 2005.11.24 R30-M1-NC-I:J18-U01 2005-11-24-06.09.49.372377 R30-M1-NC-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:47283, Link has been severed<|endoftext|>1132841389 2005.11.24 R22-M0-NC-I:J18-U01 2005-11-24-06.09.49.518547 R22-M0-NC-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f5<|endoftext|>1132841389 2005.11.24 R22-M0-NC-I:J18-U01 2005-11-24-06.09.49.656625 R22-M0-NC-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:47181, Link has been severed<|endoftext|>1132841389 2005.11.24 R23-M1-NC-I:J18-U01 2005-11-24-06.09.49.822844 R23-M1-NC-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2<|endoftext|>1132841389 2005.11.24 R23-M1-NC-I:J18-U01 2005-11-24-06.09.49.962257 R23-M1-NC-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:47204, Link has been severed<|endoftext|>\n",
      "Number of logs in this session: 10\n",
      "\n",
      "Session 4:\n",
      "Input IDs: [27, 8190, 26661, 23654, 16, 220, 1049, 20, 13, 605, 13, 966, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1691, 13, 17735, 26067, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 24, 11, 520, 220, 15, 87, 972, 67, 19852, 66, 15, 11, 7056, 220, 15, 87, 605, 100257, 8190, 26661, 23654, 16, 220, 1049, 20, 13, 605, 13, 966, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1691, 13, 20691, 11739, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 972, 11, 520, 220, 15, 87, 16, 65, 21330, 17814, 11, 7056, 220, 15, 87, 1272, 100257, 8190, 26661, 23654, 16, 220, 1049, 20, 13, 605, 13, 966, 432, 843, 5364, 15, 11500, 33, 7813, 25, 41, 717, 35681, 806, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1691, 13, 23292, 25828, 432, 843, 5364, 15, 11500, 33, 7813, 25, 41, 717, 35681, 806, 432, 1950, 80791, 31871, 27809, 8045, 220, 2148, 11, 520, 220, 15, 87, 2371, 66, 17, 6194, 1399, 11, 7056, 220, 15, 87, 508, 100257, 8190, 26661, 23654, 16, 220, 1049, 20, 13, 605, 13, 966, 432, 2705, 5364, 15, 11500, 24, 7813, 25, 41, 2437, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1691, 13, 24832, 23545, 432, 2705, 5364, 15, 11500, 24, 7813, 25, 41, 2437, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 1682, 11, 520, 220, 15, 87, 16, 65, 24758, 21251, 11, 7056, 220, 15, 87, 2437, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 1041, 19242, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 24, 11, 520, 220, 15, 87, 972, 67, 19852, 66, 15, 11, 7056, 220, 15, 87, 605, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 14057, 26563, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 972, 11, 520, 220, 15, 87, 16, 65, 21330, 17814, 11, 7056, 220, 15, 87, 1272, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 843, 5364, 15, 11500, 33, 7813, 25, 41, 717, 35681, 806, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 17153, 13364, 432, 843, 5364, 15, 11500, 33, 7813, 25, 41, 717, 35681, 806, 432, 1950, 80791, 31871, 27809, 8045, 220, 2148, 11, 520, 220, 15, 87, 2371, 66, 17, 6194, 1399, 11, 7056, 220, 15, 87, 508, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 2705, 5364, 15, 11500, 24, 7813, 25, 41, 2437, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 19305, 20275, 432, 2705, 5364, 15, 11500, 24, 7813, 25, 41, 2437, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 1682, 11, 520, 220, 15, 87, 16, 65, 24758, 21251, 11, 7056, 220, 15, 87, 2437, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 24939, 18058, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 24, 11, 520, 220, 15, 87, 972, 67, 19852, 66, 15, 11, 7056, 220, 15, 87, 605, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 18248, 22455, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 972, 11, 520, 220, 15, 87, 16, 65, 21330, 17814, 11, 7056, 220, 15, 87, 1272, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "Session Label: 0\n",
      "Decoded Log: <1130686581 2005.10.30 R57-M0-NE-C:J15-U01 2005-10-30-07.36.21.503956 R57-M0-NE-C:J15-U01 RAS KERNEL INFO CE sym 9, at 0x18d431c0, mask 0x10<|endoftext|>1130686581 2005.10.30 R20-M0-N4-C:J09-U01 2005-10-30-07.36.21.588169 R20-M0-N4-C:J09-U01 RAS KERNEL INFO CE sym 18, at 0x1b469680, mask 0x40<|endoftext|>1130686581 2005.10.30 R32-M0-NB-C:J12-U11 2005-10-30-07.36.21.688918 R32-M0-NB-C:J12-U11 RAS KERNEL INFO CE sym 31, at 0x04c2bb60, mask 0x20<|endoftext|>1130686581 2005.10.30 R06-M0-N9-C:J02-U01 2005-10-30-07.36.21.796051 R06-M0-N9-C:J02-U01 RAS KERNEL INFO CE sym 29, at 0x1b719940, mask 0x02<|endoftext|>1130686582 2005.10.30 R57-M0-NE-C:J15-U01 2005-10-30-07.36.22.100562 R57-M0-NE-C:J15-U01 RAS KERNEL INFO CE sym 9, at 0x18d431c0, mask 0x10<|endoftext|>1130686582 2005.10.30 R20-M0-N4-C:J09-U01 2005-10-30-07.36.22.226846 R20-M0-N4-C:J09-U01 RAS KERNEL INFO CE sym 18, at 0x1b469680, mask 0x40<|endoftext|>1130686582 2005.10.30 R32-M0-NB-C:J12-U11 2005-10-30-07.36.22.334305 R32-M0-NB-C:J12-U11 RAS KERNEL INFO CE sym 31, at 0x04c2bb60, mask 0x20<|endoftext|>1130686582 2005.10.30 R06-M0-N9-C:J02-U01 2005-10-30-07.36.22.435922 R06-M0-N9-C:J02-U01 RAS KERNEL INFO CE sym 29, at 0x1b719940, mask 0x02<|endoftext|>1130686582 2005.10.30 R57-M0-NE-C:J15-U01 2005-10-30-07.36.22.735408 R57-M0-NE-C:J15-U01 RAS KERNEL INFO CE sym 9, at 0x18d431c0, mask 0x10<|endoftext|>1130686582 2005.10.30 R20-M0-N4-C:J09-U01 2005-10-30-07.36.22.820866 R20-M0-N4-C:J09-U01 RAS KERNEL INFO CE sym 18, at 0x1b469680, mask 0x40<|endoftext|>\n",
      "Number of logs in this session: 10\n",
      "\n",
      "Session 5:\n",
      "Input IDs: [27, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 17416, 24132, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 19808, 24288, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 15894, 17470, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 24239, 11128, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 20775, 21112, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 24495, 21138, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 22644, 9741, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 23409, 23574, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 21598, 24531, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 23480, 15901, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "Session Label: 0\n",
      "Decoded Log: <1127243551 2005.09.20 NULL 2005-09-20-12.12.31.620661 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.622813 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.625326 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.629179 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.632740 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.638609 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.647185 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.651628 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.653793 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.657014 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>\n",
      "Number of logs in this session: 10\n",
      "Average tokens per session: 693.82\n",
      "Max tokens in a session: 2549\n",
      "=========== Start of Validate Sessions =====================\n",
      "Number of sessions: 508403\n",
      "\n",
      "Session 1:\n",
      "Input IDs: [27, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1032, 35681, 806, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 26563, 22708, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1032, 35681, 806, 432, 1950, 80791, 31871, 24038, 6332, 13, 8929, 5547, 100257, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1114, 35681, 806, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 25747, 12112, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1114, 35681, 806, 432, 1950, 80791, 31871, 24038, 6332, 13, 8929, 1399, 100257, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2304, 35681, 1721, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 22889, 23776, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2304, 35681, 1721, 432, 1950, 80791, 31871, 24038, 6332, 13, 8929, 2618, 100257, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2839, 35681, 1721, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 26026, 5120, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2839, 35681, 1721, 432, 1950, 80791, 31871, 24038, 6332, 13, 10161, 2946, 100257, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2304, 35681, 806, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 24597, 20615, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2304, 35681, 806, 432, 1950, 80791, 31871, 24038, 6332, 13, 8929, 5495, 100257, 7261, 15966, 25921, 23, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2839, 35681, 806, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2318, 13, 25476, 20077, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2839, 35681, 806, 432, 1950, 80791, 31871, 24038, 6332, 13, 10161, 2075, 100257, 7261, 15966, 25921, 24, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2589, 35681, 806, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2545, 13, 11030, 11256, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 2589, 35681, 806, 432, 1950, 80791, 31871, 24038, 6332, 13, 10161, 5728, 100257, 7261, 15966, 25921, 24, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 868, 35681, 1721, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2545, 13, 21040, 23642, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 868, 35681, 1721, 432, 1950, 80791, 31871, 24038, 6332, 13, 10161, 3487, 100257, 7261, 15966, 25921, 24, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1114, 35681, 1721, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2545, 13, 27033, 20478, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 1114, 35681, 1721, 432, 1950, 80791, 31871, 24038, 6332, 13, 8929, 2096, 100257, 7261, 15966, 25921, 24, 220, 1049, 20, 13, 2589, 13, 1591, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 806, 35681, 1721, 220, 1049, 20, 12, 2589, 12, 1591, 12, 717, 13, 3487, 13, 2545, 13, 24646, 24495, 432, 508, 5364, 15, 11500, 17, 7813, 25, 41, 806, 35681, 1721, 432, 1950, 80791, 31871, 24038, 6332, 13, 10161, 3226, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "Session Label: 0\n",
      "Decoded Log: <1122580568 2005.07.28 R20-M0-N2-C:J13-U11 2005-07-28-12.56.08.846712 R20-M0-N2-C:J13-U11 RAS KERNEL INFO generating core.14461<|endoftext|>1122580568 2005.07.28 R20-M0-N2-C:J17-U11 2005-07-28-12.56.08.873215 R20-M0-N2-C:J17-U11 RAS KERNEL INFO generating core.14460<|endoftext|>1122580568 2005.07.28 R20-M0-N2-C:J05-U01 2005-07-28-12.56.08.899757 R20-M0-N2-C:J05-U01 RAS KERNEL INFO generating core.14447<|endoftext|>1122580568 2005.07.28 R20-M0-N2-C:J03-U01 2005-07-28-12.56.08.926110 R20-M0-N2-C:J03-U01 RAS KERNEL INFO generating core.14959<|endoftext|>1122580568 2005.07.28 R20-M0-N2-C:J05-U11 2005-07-28-12.56.08.952590 R20-M0-N2-C:J05-U11 RAS KERNEL INFO generating core.14463<|endoftext|>1122580568 2005.07.28 R20-M0-N2-C:J03-U11 2005-07-28-12.56.08.979394 R20-M0-N2-C:J03-U11 RAS KERNEL INFO generating core.14975<|endoftext|>1122580569 2005.07.28 R20-M0-N2-C:J07-U11 2005-07-28-12.56.09.006178 R20-M0-N2-C:J07-U11 RAS KERNEL INFO generating core.14974<|endoftext|>1122580569 2005.07.28 R20-M0-N2-C:J15-U01 2005-07-28-12.56.09.032569 R20-M0-N2-C:J15-U01 RAS KERNEL INFO generating core.14956<|endoftext|>1122580569 2005.07.28 R20-M0-N2-C:J17-U01 2005-07-28-12.56.09.059064 R20-M0-N2-C:J17-U01 RAS KERNEL INFO generating core.14444<|endoftext|>1122580569 2005.07.28 R20-M0-N2-C:J11-U01 2005-07-28-12.56.09.085638 R20-M0-N2-C:J11-U01 RAS KERNEL INFO generating core.14957<|endoftext|>\n",
      "Number of logs in this session: 10\n",
      "\n",
      "Session 2:\n",
      "Input IDs: [27, 5037, 22869, 21144, 22, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1806, 13, 18517, 12994, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 22, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1806, 13, 20936, 24491, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 22, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1806, 13, 19774, 22922, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 22, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1806, 13, 24344, 14205, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 22, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1806, 13, 16415, 22058, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 23, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1987, 13, 8190, 22904, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 23, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1987, 13, 11584, 23505, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 23, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1987, 13, 20153, 12652, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 23, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1987, 13, 22048, 19027, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257, 5037, 22869, 21144, 23, 220, 1049, 20, 13, 2705, 13, 806, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 220, 1049, 20, 12, 2705, 12, 806, 12, 1313, 13, 2137, 13, 1987, 13, 25388, 26223, 432, 966, 5364, 15, 11500, 24, 7813, 25, 41, 845, 35681, 1721, 432, 1950, 80791, 435, 31174, 828, 30715, 33, 1493, 12956, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "Session Label: 1\n",
      "Decoded Log: <1118554777 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.37.424233 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554777 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.37.522663 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554777 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.37.667613 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554777 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.37.844246 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554777 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.37.960541 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554778 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.38.113833 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554778 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.38.214907 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554778 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.38.433209 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554778 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.38.546710 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>1118554778 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.39.38.697834 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt<|endoftext|>\n",
      "Number of logs in this session: 10\n",
      "\n",
      "Session 3:\n",
      "Input IDs: [27, 8190, 17058, 10350, 23, 220, 1049, 20, 13, 806, 13, 1187, 432, 1958, 5364, 15, 11500, 23, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2166, 13, 23103, 22801, 432, 1958, 5364, 15, 11500, 23, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 18395, 435, 31174, 272, 3205, 25, 4703, 5403, 1984, 9436, 389, 356, 822, 3103, 7728, 311, 220, 10861, 13, 845, 13, 4161, 13, 8027, 25, 21505, 1806, 11, 6074, 706, 1027, 84450, 100257, 8190, 17058, 10350, 23, 220, 1049, 20, 13, 806, 13, 1187, 432, 1644, 5364, 15, 11500, 23, 22197, 25, 41, 972, 35681, 806, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2166, 13, 25110, 17763, 432, 1644, 5364, 15, 11500, 23, 22197, 25, 41, 972, 35681, 806, 432, 1950, 80791, 31871, 272, 3205, 25, 39517, 8450, 220, 868, 11, 2082, 28, 15, 11, 28053, 28, 15, 11, 2686, 28, 15, 87, 931, 4119, 69, 17, 100257, 8190, 17058, 10350, 23, 220, 1049, 20, 13, 806, 13, 1187, 432, 1958, 5364, 16, 11500, 23, 22197, 25, 41, 972, 35681, 806, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2166, 13, 21278, 18775, 432, 1958, 5364, 16, 11500, 23, 22197, 25, 41, 972, 35681, 806, 432, 1950, 80791, 31871, 272, 3205, 25, 39517, 8450, 220, 868, 11, 2082, 28, 15, 11, 28053, 28, 15, 11, 2686, 28, 15, 87, 931, 4119, 69, 17, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 806, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 24254, 22210, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 806, 432, 1950, 80791, 31871, 272, 3205, 25, 39517, 8450, 220, 868, 11, 2082, 28, 15, 11, 28053, 28, 15, 11, 2686, 28, 15, 87, 931, 4119, 69, 17, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 806, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 14057, 5245, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 806, 432, 1950, 18395, 435, 31174, 272, 3205, 25, 4703, 5403, 1984, 9436, 389, 356, 822, 3103, 7728, 311, 220, 10861, 13, 845, 13, 4161, 13, 8027, 25, 21757, 5833, 11, 6074, 706, 1027, 84450, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 17662, 10898, 432, 966, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 18395, 435, 31174, 272, 3205, 25, 4703, 5403, 1984, 9436, 389, 356, 822, 3103, 7728, 311, 220, 10861, 13, 845, 13, 4161, 13, 8027, 25, 21757, 6069, 11, 6074, 706, 1027, 84450, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 1313, 5364, 15, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 21312, 23215, 432, 1313, 5364, 15, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 80791, 31871, 272, 3205, 25, 39517, 8450, 220, 868, 11, 2082, 28, 15, 11, 28053, 28, 15, 11, 2686, 28, 15, 87, 931, 4119, 69, 20, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 1313, 5364, 15, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 20744, 15894, 432, 1313, 5364, 15, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 18395, 435, 31174, 272, 3205, 25, 4703, 5403, 1984, 9436, 389, 356, 822, 3103, 7728, 311, 220, 10861, 13, 845, 13, 4161, 13, 8027, 25, 20617, 5932, 11, 6074, 706, 1027, 84450, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 1419, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 23105, 24344, 432, 1419, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 80791, 31871, 272, 3205, 25, 39517, 8450, 220, 868, 11, 2082, 28, 15, 11, 28053, 28, 15, 11, 2686, 28, 15, 87, 931, 4119, 69, 17, 100257, 8190, 17058, 10350, 24, 220, 1049, 20, 13, 806, 13, 1187, 432, 1419, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 220, 1049, 20, 12, 806, 12, 1187, 12, 2705, 13, 2545, 13, 2491, 13, 26366, 15574, 432, 1419, 5364, 16, 12, 10153, 22197, 25, 41, 972, 35681, 1721, 432, 1950, 18395, 435, 31174, 272, 3205, 25, 4703, 5403, 1984, 9436, 389, 356, 822, 3103, 7728, 311, 220, 10861, 13, 845, 13, 4161, 13, 8027, 25, 21757, 2371, 11, 6074, 706, 1027, 84450, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "Session Label: 1\n",
      "Decoded Log: <1132841388 2005.11.24 R34-M0-N8-I:J18-U01 2005-11-24-06.09.48.627623 R34-M0-N8-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:47337, Link has been severed<|endoftext|>1132841388 2005.11.24 R33-M0-N8-I:J18-U11 2005-11-24-06.09.48.761416 R33-M0-N8-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2<|endoftext|>1132841388 2005.11.24 R34-M1-N8-I:J18-U11 2005-11-24-06.09.48.909367 R34-M1-N8-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2<|endoftext|>1132841389 2005.11.24 R30-M1-NC-I:J18-U11 2005-11-24-06.09.49.063564 R30-M1-NC-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2<|endoftext|>1132841389 2005.11.24 R30-M1-NC-I:J18-U11 2005-11-24-06.09.49.226180 R30-M1-NC-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:47284, Link has been severed<|endoftext|>1132841389 2005.11.24 R30-M1-NC-I:J18-U01 2005-11-24-06.09.49.372377 R30-M1-NC-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:47283, Link has been severed<|endoftext|>1132841389 2005.11.24 R22-M0-NC-I:J18-U01 2005-11-24-06.09.49.518547 R22-M0-NC-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f5<|endoftext|>1132841389 2005.11.24 R22-M0-NC-I:J18-U01 2005-11-24-06.09.49.656625 R22-M0-NC-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:47181, Link has been severed<|endoftext|>1132841389 2005.11.24 R23-M1-NC-I:J18-U01 2005-11-24-06.09.49.822844 R23-M1-NC-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2<|endoftext|>1132841389 2005.11.24 R23-M1-NC-I:J18-U01 2005-11-24-06.09.49.962257 R23-M1-NC-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:47204, Link has been severed<|endoftext|>\n",
      "Number of logs in this session: 10\n",
      "\n",
      "Session 4:\n",
      "Input IDs: [27, 8190, 26661, 23654, 16, 220, 1049, 20, 13, 605, 13, 966, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1691, 13, 17735, 26067, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 24, 11, 520, 220, 15, 87, 972, 67, 19852, 66, 15, 11, 7056, 220, 15, 87, 605, 100257, 8190, 26661, 23654, 16, 220, 1049, 20, 13, 605, 13, 966, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1691, 13, 20691, 11739, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 972, 11, 520, 220, 15, 87, 16, 65, 21330, 17814, 11, 7056, 220, 15, 87, 1272, 100257, 8190, 26661, 23654, 16, 220, 1049, 20, 13, 605, 13, 966, 432, 843, 5364, 15, 11500, 33, 7813, 25, 41, 717, 35681, 806, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1691, 13, 23292, 25828, 432, 843, 5364, 15, 11500, 33, 7813, 25, 41, 717, 35681, 806, 432, 1950, 80791, 31871, 27809, 8045, 220, 2148, 11, 520, 220, 15, 87, 2371, 66, 17, 6194, 1399, 11, 7056, 220, 15, 87, 508, 100257, 8190, 26661, 23654, 16, 220, 1049, 20, 13, 605, 13, 966, 432, 2705, 5364, 15, 11500, 24, 7813, 25, 41, 2437, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1691, 13, 24832, 23545, 432, 2705, 5364, 15, 11500, 24, 7813, 25, 41, 2437, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 1682, 11, 520, 220, 15, 87, 16, 65, 24758, 21251, 11, 7056, 220, 15, 87, 2437, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 1041, 19242, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 24, 11, 520, 220, 15, 87, 972, 67, 19852, 66, 15, 11, 7056, 220, 15, 87, 605, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 14057, 26563, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 972, 11, 520, 220, 15, 87, 16, 65, 21330, 17814, 11, 7056, 220, 15, 87, 1272, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 843, 5364, 15, 11500, 33, 7813, 25, 41, 717, 35681, 806, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 17153, 13364, 432, 843, 5364, 15, 11500, 33, 7813, 25, 41, 717, 35681, 806, 432, 1950, 80791, 31871, 27809, 8045, 220, 2148, 11, 520, 220, 15, 87, 2371, 66, 17, 6194, 1399, 11, 7056, 220, 15, 87, 508, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 2705, 5364, 15, 11500, 24, 7813, 25, 41, 2437, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 19305, 20275, 432, 2705, 5364, 15, 11500, 24, 7813, 25, 41, 2437, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 1682, 11, 520, 220, 15, 87, 16, 65, 24758, 21251, 11, 7056, 220, 15, 87, 2437, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 24939, 18058, 432, 3226, 5364, 15, 12, 4031, 7813, 25, 41, 868, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 24, 11, 520, 220, 15, 87, 972, 67, 19852, 66, 15, 11, 7056, 220, 15, 87, 605, 100257, 8190, 26661, 23654, 17, 220, 1049, 20, 13, 605, 13, 966, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 220, 1049, 20, 12, 605, 12, 966, 12, 2589, 13, 1927, 13, 1313, 13, 18248, 22455, 432, 508, 5364, 15, 11500, 19, 7813, 25, 41, 2545, 35681, 1721, 432, 1950, 80791, 31871, 27809, 8045, 220, 972, 11, 520, 220, 15, 87, 16, 65, 21330, 17814, 11, 7056, 220, 15, 87, 1272, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "Session Label: 0\n",
      "Decoded Log: <1130686581 2005.10.30 R57-M0-NE-C:J15-U01 2005-10-30-07.36.21.503956 R57-M0-NE-C:J15-U01 RAS KERNEL INFO CE sym 9, at 0x18d431c0, mask 0x10<|endoftext|>1130686581 2005.10.30 R20-M0-N4-C:J09-U01 2005-10-30-07.36.21.588169 R20-M0-N4-C:J09-U01 RAS KERNEL INFO CE sym 18, at 0x1b469680, mask 0x40<|endoftext|>1130686581 2005.10.30 R32-M0-NB-C:J12-U11 2005-10-30-07.36.21.688918 R32-M0-NB-C:J12-U11 RAS KERNEL INFO CE sym 31, at 0x04c2bb60, mask 0x20<|endoftext|>1130686581 2005.10.30 R06-M0-N9-C:J02-U01 2005-10-30-07.36.21.796051 R06-M0-N9-C:J02-U01 RAS KERNEL INFO CE sym 29, at 0x1b719940, mask 0x02<|endoftext|>1130686582 2005.10.30 R57-M0-NE-C:J15-U01 2005-10-30-07.36.22.100562 R57-M0-NE-C:J15-U01 RAS KERNEL INFO CE sym 9, at 0x18d431c0, mask 0x10<|endoftext|>1130686582 2005.10.30 R20-M0-N4-C:J09-U01 2005-10-30-07.36.22.226846 R20-M0-N4-C:J09-U01 RAS KERNEL INFO CE sym 18, at 0x1b469680, mask 0x40<|endoftext|>1130686582 2005.10.30 R32-M0-NB-C:J12-U11 2005-10-30-07.36.22.334305 R32-M0-NB-C:J12-U11 RAS KERNEL INFO CE sym 31, at 0x04c2bb60, mask 0x20<|endoftext|>1130686582 2005.10.30 R06-M0-N9-C:J02-U01 2005-10-30-07.36.22.435922 R06-M0-N9-C:J02-U01 RAS KERNEL INFO CE sym 29, at 0x1b719940, mask 0x02<|endoftext|>1130686582 2005.10.30 R57-M0-NE-C:J15-U01 2005-10-30-07.36.22.735408 R57-M0-NE-C:J15-U01 RAS KERNEL INFO CE sym 9, at 0x18d431c0, mask 0x10<|endoftext|>1130686582 2005.10.30 R20-M0-N4-C:J09-U01 2005-10-30-07.36.22.820866 R20-M0-N4-C:J09-U01 RAS KERNEL INFO CE sym 18, at 0x1b469680, mask 0x40<|endoftext|>\n",
      "Number of logs in this session: 10\n",
      "\n",
      "Session 5:\n",
      "Input IDs: [27, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 17416, 24132, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 19808, 24288, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 15894, 17470, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 24239, 11128, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 20775, 21112, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 24495, 21138, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 22644, 9741, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 23409, 23574, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 21598, 24531, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257, 7261, 24735, 17306, 16, 220, 1049, 20, 13, 2545, 13, 508, 1808, 220, 1049, 20, 12, 2545, 12, 508, 12, 717, 13, 717, 13, 2148, 13, 23480, 15901, 1808, 432, 1950, 22403, 6546, 13170, 887, 47026, 4223, 2042, 4295, 12486, 3044, 25, 12486, 7645, 28, 15, 8922, 1052, 47122, 354, 34489, 403, 49392, 7356, 8922, 1584, 28, 6849, 18, 5830, 16972, 358, 3055, 28660, 26648, 487, 11764, 17093, 57024, 78, 52, 9996, 26648, 12594, 426, 6200, 95352, 47, 402, 6687, 3849, 100257]\n",
      "Segment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "Session Label: 0\n",
      "Decoded Log: <1127243551 2005.09.20 NULL 2005-09-20-12.12.31.620661 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.622813 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.625326 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.629179 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.632740 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.638609 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.647185 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.651628 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.653793 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>1127243551 2005.09.20 NULL 2005-09-20-12.12.31.657014 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)<|endoftext|>\n",
      "Number of logs in this session: 10\n",
      "Average tokens per session: 693.82\n",
      "Max tokens in a session: 2549\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "# This cell read the public log dataset from /logs/BGL.log.\n",
    "# Tokenizing the logs and creating input features for the model.\n",
    "# Input features was splitted as test set (70%), validation set (15%) \n",
    "# and test set (15%)\n",
    "##########################################################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score as f1_score_sklearn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import random\n",
    "\n",
    "max_token_length = 4096 # set a initial value for max token length\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "# Define Log Dataset\n",
    "class LogDataset(Dataset):\n",
    "    def __init__(self, sessions):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with a flat list of logs.\n",
    "\n",
    "        Parameters:\n",
    "            sessions: List of dictionaries with keys `input_ids`, `segment_ids`, and `session_label`.\n",
    "        \"\"\"\n",
    "        self.sessions = sessions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        session = self.sessions[idx]\n",
    "        return session \n",
    "\n",
    "\n",
    "# Define clean function\n",
    "def clean(text):\n",
    "    \"\"\"Cleans a log message by removing special characters, and extra spaces.\"\"\"\n",
    "    import re\n",
    "    import string\n",
    "\n",
    "    text = re.sub(r'\\]|\\[|\\)|\\(|\\=|\\,|\\;', ' ', text)  # Remove specific symbols [ ] ( ) = , and ;\n",
    "    text = \" \".join([word.lower() if word.isupper() else word for word in text.strip().split()])\n",
    "    text = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', text))\n",
    "    return \" \".join([word.lower().strip() for word in text.strip().split()])\n",
    "\n",
    "\n",
    "\n",
    "# function to load the pre-trained GPT4 BPE tokenizer \n",
    "def load_gpt4_tokenizer():\n",
    "    print(\"Loading cl100k_base (GPT-4) tokenizer...\")\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# Tokenize logs with the pre-trained GPT-4 tokenizer \n",
    "def tokenize_and_construct_input(log_sequence, tokenizer, max_len=max_token_length):\n",
    "    \"\"\"\n",
    "    Tokenize log messages and construct input IDs and segment IDs.\n",
    "    \n",
    "    Parameters:\n",
    "        log_sequence: List of log messages in a sequence.\n",
    "        tokenizer: Pre-trained tokenizer  \n",
    "    \n",
    "    Returns:\n",
    "        input_ids: List of token IDs for the entire sequence.\n",
    "        segment_ids: List of segment IDs corresponding to each token.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    segment_ids = []\n",
    "\n",
    "    allowed_special = {\"<|startoftext|>\", \"<|endoftext|>\"}\n",
    "    bos_token = tokenizer.encode(\"<|startoftext|>\", allowed_special=allowed_special)[0] # get the token id of <|startoftext|>\n",
    "    eos_token = tokenizer.encode(\"<|endoftext|>\", allowed_special=allowed_special)[0] # get the token id of <|endoftext|>\n",
    "\n",
    "\n",
    "    # Add bos_token and eos_token markers to the log sequence\n",
    "    \n",
    "    for i, log in enumerate(log_sequence):\n",
    "        # Tokenize the log message\n",
    "        tokens = tokenizer.encode(log, allowed_special={\"<|startoftext|>\", \"<|endoftext|>\"})\n",
    "        \n",
    "        # Add bos_token to the first log only in the sequence\n",
    "        if i == 0:  # First log in sequence\n",
    "            tokens = [bos_token] + tokens  # <|startoftext|> + log tokens\n",
    "        tokens = tokens + [eos_token]  # <|endoftext|> appended to the log tokens\n",
    "        \n",
    "        # Append tokens to input_ids\n",
    "        input_ids.extend(tokens)\n",
    "        \n",
    "        # Create segment IDs for the current log\n",
    "        segment_ids.extend([i] * len(tokens))  # Segment ID equals log position\n",
    "\n",
    "    #if len(input_ids) > max_len:\n",
    "    #    print(f\"Warning: Sequence length {len(input_ids)} exceeds max length {max_len}. Truncating.\")\n",
    "\n",
    "        # Truncate to max_len\n",
    "        input_ids = input_ids[:max_len]\n",
    "        segment_ids = segment_ids[:max_len]\n",
    "\n",
    "    return input_ids, segment_ids\n",
    "\n",
    "\n",
    "\n",
    "def oversample_with_ratio_control(sessions, beta):\n",
    "    \"\"\"\n",
    "    Oversample the minority class to achieve a specific proportion (beta) in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        sessions: List of dictionaries containing `session_label`.\n",
    "        beta: Desired proportion of the minority class in the oversampled dataset.\n",
    "\n",
    "    Returns:\n",
    "        oversampled_sessions: Balanced list of dictionaries.\n",
    "    \"\"\"\n",
    "    normal_sessions = [s for s in sessions if s[\"session_label\"] == 0]\n",
    "    anomalous_sessions = [s for s in sessions if s[\"session_label\"] == 1]\n",
    "\n",
    "    normal_count = len(normal_sessions)\n",
    "    anomalous_count = len(anomalous_sessions)\n",
    "    total_count = normal_count + anomalous_count\n",
    "\n",
    "    if anomalous_count == 0:\n",
    "        raise ValueError(\"No samples found in the minority class (anomalous logs).\")\n",
    "\n",
    "    alpha = anomalous_count / total_count\n",
    "\n",
    "    target_minority_size = int((beta * (1 - alpha) / (1 - beta)) * total_count) \n",
    "\n",
    "    new_added_anomalous = target_minority_size - anomalous_count\n",
    "\n",
    "    if target_minority_size <= anomalous_count:\n",
    "        print(f\"No oversampling needed. Current minority size meets the target proportion.\")\n",
    "        return sessions\n",
    "\n",
    "    # Perform oversampling with replacement\n",
    "    new_added_anomalous = np.random.choice(anomalous_sessions, size=new_added_anomalous, replace=True)\n",
    "    oversampled_sessions = normal_sessions + anomalous_sessions + list(new_added_anomalous)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    np.random.shuffle(oversampled_sessions)\n",
    "\n",
    "    print(f\"Original Minority Samples: {anomalous_count}\")\n",
    "    print(f\"New Target Minority Size: {target_minority_size}\")\n",
    "    print(f\"New Total Samples: {len(oversampled_sessions)}\")\n",
    "\n",
    "    return oversampled_sessions\n",
    "\n",
    "   \n",
    "def create_sessions_with_segment_ids(log_data, tokenizer, windows_size, step_size):\n",
    "    \"\"\"\n",
    "    Process log data into sessions with input IDs and segment IDs.\n",
    "\n",
    "    Parameters:\n",
    "        log_data: List of log messages.\n",
    "        tokenizer: Pre-trained tokenizer.\n",
    "        windows_size: Number of logs in a sliding window.\n",
    "        step_size: Step size for sliding window.\n",
    "\n",
    "    Returns:\n",
    "        sessions: List of dictionaries containing input IDs and segment IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    sessions = []\n",
    "    print(\"Creating sessions...\")\n",
    "    for i in tqdm(range(0, len(log_data) - windows_size, step_size), desc=\"Processing Sessions\"):\n",
    "        logs_in_session = []  # Reset logs_in_session for each new session\n",
    "        label = 0  # Initialize as the session label\n",
    "        for j in range(i, i + windows_size):\n",
    "            content = log_data[j]\n",
    "\n",
    "            # Check for an anomaly and update the session label\n",
    "            if content[0] != \"-\":  # If the log doesn't start with \"-\", mark the session as anomalous\n",
    "                label = 1\n",
    "            # remove label from log messages\n",
    "            content = content[content.find(' ') + 1:]\n",
    "            if clean_log == True:\n",
    "                content = clean(content.lower())  # Clean the log\n",
    "            logs_in_session.append(content)\n",
    "\n",
    "        # Construct input IDs and segment IDs\n",
    "        input_ids, segment_ids = tokenize_and_construct_input(logs_in_session, tokenizer)\n",
    "        # Append the session as a dictionary to the sessions list\n",
    "        sessions.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"segment_ids\": segment_ids,\n",
    "            \"session_label\": label\n",
    "        })\n",
    "\n",
    "    return sessions\n",
    "\n",
    "\n",
    "# Updated load_supercomputers function with GPT-2 BPE\n",
    "def load_supercomputers_with_gpt2_bpe_stratified(\n",
    "    log_file, \n",
    "    train_ratio=0.7, \n",
    "    windows_size=20, \n",
    "    step_size=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Load logs, create sessions, then perform a stratified train/test split \n",
    "    based on session labels (normal vs. anomalous).\n",
    "    \n",
    "    Parameters:\n",
    "        log_file: Path to the log file.\n",
    "        train_ratio: Ratio of training data, e.g. 0.7 = 70% for training.\n",
    "        windows_size: Window size for sliding window.\n",
    "        step_size: Step size for the sliding window.\n",
    "\n",
    "    Returns:\n",
    "        training_sessions: List of dictionaries for training logs.\n",
    "        test_sessions: List of dictionaries for test logs.\n",
    "        tokenizer: Pre-trained GPT-2 tokenizer.\n",
    "    \"\"\"\n",
    "    print(\"Loading logs from:\", log_file)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1) Read all logs\n",
    "    with open(log_file, mode=\"r\", encoding='utf8') as f:\n",
    "       logs = [x.strip() for x in tqdm(f, desc=\"Reading Logs\")]\n",
    "\n",
    "    print(f\"Loaded {len(logs)} logs in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    # 2) Load GPT4 tokenizer\n",
    "    tokenizer = load_gpt4_tokenizer()\n",
    "\n",
    "    # 3) Create sessions from ALL logs\n",
    "    #    Instead of splitting first, we create sessions for the entire log file.\n",
    "    all_sessions = create_sessions_with_segment_ids(logs, tokenizer, windows_size, step_size)\n",
    "\n",
    "    \n",
    "    # 4) calculate the number of length that fit xx% of the sessions \n",
    "    # for testing the choosen of max token length\n",
    "    token_lengths = [len(session[\"input_ids\"]) for session in all_sessions]\n",
    "    max_len_90 = int(np.percentile(token_lengths, 90))\n",
    "    max_len_95 = int(np.percentile(token_lengths, 95))\n",
    "    max_token_length = max(token_lengths)\n",
    "    print(f\"Max tokens in 90% of sessions: {max_len_90}\")\n",
    "    print(f\"Max tokens in 95% of sessions: {max_len_95}\")\n",
    "    print(f\"Max tokens in 100% of sessions: {max_token_length}\") \n",
    "    \n",
    "\n",
    "    # 5) Extract labels for each session\n",
    "    session_labels = [s[\"session_label\"] for s in all_sessions]\n",
    "\n",
    "    # 6) Perform stratified split\n",
    "    train_sessions, temp_sessions = train_test_split(\n",
    "        all_sessions,\n",
    "        test_size=(1 - train_ratio),\n",
    "        stratify=session_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Further split the temp_sessions into validation and test sets\n",
    "    val_ratio = 0.5  # 50% of the temp_sessions for validation\n",
    "    val_sessions, test_sessions = train_test_split(\n",
    "        temp_sessions,\n",
    "        test_size=val_ratio,\n",
    "        stratify=[s[\"session_label\"] for s in temp_sessions],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Number of sessions after sliding window: {len(all_sessions)}\")\n",
    "    print(f\"Train sessions: {len(train_sessions)} | Validation sessions: {len(val_sessions)} | Test sessions: {len(test_sessions)}\")\n",
    "\n",
    "    # double check the data pipeline before oversampling \n",
    "    train_normal = sum(s['session_label'] == 0 for s in train_sessions)\n",
    "    train_anomalous = sum(s['session_label'] == 1 for s in train_sessions)\n",
    "    val_normal = sum(s['session_label'] == 0 for s in val_sessions)\n",
    "    val_anomalous = sum(s['session_label'] == 1 for s in val_sessions)\n",
    "    test_normal = sum(s['session_label'] == 0 for s in test_sessions)\n",
    "    test_anomalous = sum(s['session_label'] == 1 for s in test_sessions)\n",
    "    print(\"\\n\",\"The ratio of Normal to Animalous befor oversampling\")\n",
    "    print(\"Train set => Normal:\", train_normal, \"| Anomalous:\", train_anomalous)\n",
    "    print(f\"Anomalous ratio:  {train_anomalous/(train_anomalous+ train_normal):.2f}\")\n",
    "    print(\"Validation set => Normal:\", val_normal, \"| Anomalous:\", val_anomalous) \n",
    "    print(f\"Anomalous ratio:  {val_anomalous/(val_anomalous+ val_normal):.2f}\")\n",
    "    print(\"Test set => Normal:\", test_normal, \"| Anomalous:\", test_anomalous) \n",
    "    print(f\"Anomaloud ratio:  {test_anomalous/(test_anomalous+ test_normal):.2f}\", \"\\n\")\n",
    "    \n",
    "    # 6) Oversample only the training sessions\n",
    "    print(\"Balancing training data with oversampling...\")\n",
    "    train_sessions = oversample_with_ratio_control(train_sessions, beta)\n",
    "\n",
    "    # double check the data pipeline after oversampling \n",
    "    train_normal = sum(s['session_label'] == 0 for s in train_sessions)\n",
    "    train_anomalous = sum(s['session_label'] == 1 for s in train_sessions)\n",
    "    train_total = train_normal + train_anomalous\n",
    "    val_normal = sum(s['session_label'] == 0 for s in val_sessions)\n",
    "    val_anomalous = sum(s['session_label'] == 1 for s in val_sessions)\n",
    "    val_total = val_normal + val_anomalous\n",
    "    test_normal = sum(s['session_label'] == 0 for s in test_sessions)\n",
    "    test_anomalous = sum(s['session_label'] == 1 for s in test_sessions)\n",
    "    test_total = test_normal + test_anomalous\n",
    "    print(\"\\n\",\"The ratio of Normal to Animalous after oversampling\")\n",
    "    print(\"Train set => Normal:\", train_normal, \"| Anomalous:\", train_anomalous) \n",
    "    print(\"Train Anomalous ratio: \", train_anomalous/train_total)\n",
    "    print(\"Validation set => Normal:\", val_normal, \"| Anomalous:\", val_anomalous) \n",
    "    print(\"Validation Anomalous ratio: \", val_anomalous/val_total)\n",
    "    print(\"Test set => Normal:\", test_normal, \"| Anomalous:\", test_anomalous) \n",
    "    print(\"Test Anomalous ratio: \", test_anomalous/test_total , \"\\n\")\n",
    "    \n",
    "    print(f\"Balanced training data: {len(train_sessions)} samples\")\n",
    "    print(f\"Total processing time: {time.time() - start_time:.2f} seconds.\")\n",
    "    \n",
    "    return train_sessions, val_sessions, test_sessions, tokenizer\n",
    "\n",
    "\n",
    "# Check if training_sessions is created correctly\n",
    "def validate_sessions(sessions, tokenizer, windows_size):\n",
    "    \"\"\"\n",
    "    Validates the sessions by printing tokenized and decoded logs.\n",
    "\n",
    "    Parameters:\n",
    "        sessions: List of dictionaries with `input_ids`, `segment_ids`, and `session_label`.\n",
    "        tokenizer: Pre-trained tokenizer.\n",
    "        windows_size: Expected number of logs in a session (for validation).\n",
    "    \"\"\"\n",
    "    print(\"=========== Start of Validate Sessions =====================\")\n",
    "    print(f\"Number of sessions: {len(sessions)}\")\n",
    "    for idx, session in enumerate(sessions[:5]):  # Check the first 5 sessions\n",
    "        print(f\"\\nSession {idx + 1}:\")\n",
    "        print(f\"Input IDs: {session['input_ids']}\")\n",
    "        print(f\"Segment IDs: {session['segment_ids']}\")\n",
    "        print(f\"Session Label: {session['session_label']}\")\n",
    "\n",
    "        # Decode and verify tokenized logs\n",
    "        decoded_log = tokenizer.decode(session[\"input_ids\"])\n",
    "        print(f\"Decoded Log: {decoded_log}\")\n",
    "\n",
    "        # Verify the number of logs in the session\n",
    "        num_logs = session['segment_ids'][-1] + 1  # Count logs separated by <|endoftext|>\n",
    "        print(f\"Number of logs in this session: {num_logs}\")\n",
    "\n",
    "\n",
    "\n",
    "log_file = \"./logs/BGL.log\"\n",
    "windows_size = 10\n",
    "step_size = 10\n",
    "train_ratio = 0.7  # 70% of the data will be used for training\n",
    "set_seed(42)  # Set random seed for reproducibility\n",
    "beta = 0.4  # control the ratio of the minority class in the dataset , 0.2 means 20% of the dataset will be the minority class\n",
    "clean_log = False  # if True, clean the logs by removing special characters and extra spaces\n",
    "\n",
    "training_sessions, val_sessions, test_sessions, tokenizer = load_supercomputers_with_gpt2_bpe_stratified(\n",
    "    log_file, \n",
    "    train_ratio=train_ratio, \n",
    "    windows_size=windows_size, \n",
    "    step_size=step_size\n",
    ")\n",
    "\n",
    "\n",
    "# Validate sessions\n",
    "validate_sessions(training_sessions, tokenizer, windows_size)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the average and max token length from all sessions (train, val, test)\n",
    "all_sessions = training_sessions + val_sessions + test_sessions\n",
    "token_lengths = [len(session[\"input_ids\"]) for session in all_sessions]\n",
    "print(f\"Average tokens per session: {sum(token_lengths) / len(token_lengths):.2f}\")\n",
    "print(f\"Max tokens in a session: {max(token_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# implement the embedding layers \n",
    "##############################################################################\n",
    "\n",
    "cl100k_vocab_size = 100264 # GPT4 BPE\n",
    "segment_ids_size = windows_size  # for BGL\n",
    "embedding_dimension = 128\n",
    "# length_95_percentile = 891 # 95% of the sessions are under 969 tokens long\n",
    "max_token_length = max_token_length  \n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size=cl100k_vocab_size, max_seq_len=max_token_length, segment_vocab_size=segment_ids_size, embedding_dim=128):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)          # GPT vocab size\n",
    "        self.segment_embedding = nn.Embedding(segment_vocab_size, embedding_dim)  # For segment IDs\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)     # For position IDs\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, position_ids=None):\n",
    "        # Automatically generate position_ids if not provided\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0).repeat(input_ids.size(0), 1)\n",
    "        \n",
    "        E_token = self.token_embedding(input_ids)         # (batch_size, seq_len, embedding_dim)\n",
    "        E_segment = self.segment_embedding(segment_ids)   # (batch_size, seq_len, embedding_dim)\n",
    "        E_position = self.position_embedding(position_ids) # (batch_size, seq_len, embedding_dim)\n",
    "        return E_token + E_segment + E_position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Construct linear self-attention encoder via linformer\n",
    "# Final AllLinLog model\n",
    "###################################################################\n",
    "\n",
    "from linformer import Linformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "max_token_length = max_token_length\n",
    "# Updated Transformer Encoder Layer using Linformer\n",
    "class LinformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, num_heads=2, ff_hidden_dim=128, k=128, dropout=0.1):\n",
    "        \"\"\"\n",
    "        A single Linformer encoder layer.\n",
    "\n",
    "        Args:\n",
    "        - embedding_dim: Input embedding dimension.\n",
    "        - num_heads: Number of attention heads.\n",
    "        - ff_hidden_dim: Hidden dimension of the feed-forward network.\n",
    "        - k: Low-rank approximation factor for attention.\n",
    "        - dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(LinformerEncoderLayer, self).__init__()\n",
    "        self.self_attention = Linformer(\n",
    "            dim=embedding_dim,\n",
    "            seq_len=max_token_length,  # Sequence length from your max_token_length\n",
    "            depth=1,  # Single depth for this layer\n",
    "            heads=num_heads,\n",
    "            k=k,  # Low-rank approximation\n",
    "            one_kv_head=True,\n",
    "            share_kv=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embedding_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        attention_output = self.self_attention(x)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "\n",
    "        # Feed-forward network with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "# Linformer-based Transformer Encoder\n",
    "class LinformerTransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers=1, embedding_dim=128, num_heads=2, ff_hidden_dim=128, k=128, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Multi-layer Linformer-based Transformer encoder.\n",
    "\n",
    "        Args:\n",
    "        - num_layers: Number of Linformer encoder layers.\n",
    "        - embedding_dim: Input embedding dimension.\n",
    "        - num_heads: Number of attention heads.\n",
    "        - ff_hidden_dim: Hidden dimension of the feed-forward network.\n",
    "        - k: Low-rank approximation factor for attention.\n",
    "        - dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(LinformerTransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            LinformerEncoderLayer(embedding_dim, num_heads, ff_hidden_dim, k, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Final AllLinLog Model\n",
    "class AllLinLog(nn.Module):\n",
    "    def __init__(self, vocab_size=cl100k_vocab_size, max_seq_len=max_token_length, segment_vocab_size=20, embedding_dim=128, \n",
    "                 num_layers=1, num_heads=2, ff_hidden_dim=128, k=128, num_classes=2, dropout=0.1):\n",
    "        super(AllLinLog, self).__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(vocab_size, max_seq_len, segment_vocab_size, embedding_dim)\n",
    "        self.encoder = LinformerTransformerEncoder(num_layers, embedding_dim, num_heads, ff_hidden_dim, k, dropout)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)  # Final classification layer\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, position_ids, attention_mask=None):\n",
    "        embeddings = self.embedding_layer(input_ids, segment_ids, position_ids)  # (batch_size, seq_len, embedding_dim)\n",
    "        encoder_output = self.encoder(embeddings)  # (batch_size, seq_len, embedding_dim)\n",
    "        pooled_output = torch.mean(encoder_output, dim=1)  # Average pooling for session-level representation\n",
    "        logits = self.fc(pooled_output)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n",
      "running with k = 32\n",
      "Total Parameters: 13693442\n",
      "Model Size: 52.24 MB\n",
      "Total Parameters: 13693442\n",
      "Model Size: 52.24 MB\n",
      "Input IDs: torch.Size([8, 765]), Segment IDs: torch.Size([8, 765])\n",
      "Attention Masks: torch.Size([8, 765]), Session Labels: torch.Size([8])\n",
      "Epoch 1/1\n",
      "Input IDs: torch.Size([8, 765]), Segment IDs: torch.Size([8, 765])\n",
      "Attention Masks: torch.Size([8, 765]), Session Labels: torch.Size([8])\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Classification Report:\n",
      "             precision   recall f1-score  support\n",
      "Normal         0.99851  0.99621  0.99736  65366.0\n",
      "Anomalous      0.95869  0.98343  0.97090   5853.0\n",
      "accuracy       0.99516  0.99516  0.99516      NaN\n",
      "macro avg      0.97860  0.98982  0.98413  71219.0\n",
      "weighted avg   0.99524  0.99516  0.99518  71219.0\n",
      "Train Loss: 0.02906, Train Accuracy: 0.99223\n",
      "Val Loss: 0.03812, Val Accuracy: 0.99516\n",
      "Anomalous Precision: 0.95869, Anomalous Recall: 0.98343, Anomalous F1 Score: 0.97090\n",
      "Training time for one epoch: 13.41 minutes\n",
      "The Memory initial allocated beform training: 52.24 MB\n",
      "Memory Used for training in this epoch: 172.96 MB\n",
      "\n",
      "Learning Rate: 5.0e-04\n",
      "New best model saved as: best_model/best_model_20250825_192642.pth \n",
      "\n",
      "Average Training Time per Epoch: 13.408363310496012 min\n",
      "Peak GPU Memoryy Allocated: 412.44 MB\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Classification Report:\n",
      "             precision   recall f1-score  support\n",
      "Normal         0.99859  0.99622  0.99740  65366.0\n",
      "Anomalous      0.95889  0.98428  0.97142   5854.0\n",
      "accuracy       0.99524  0.99524  0.99524      NaN\n",
      "macro avg      0.97874  0.99025  0.98441  71220.0\n",
      "weighted avg   0.99533  0.99524  0.99527  71220.0\n",
      "Test evaluation result saved to results/test_classification_report_BGL_20250825_192642.csv and results/test_classification_report_BGL_20250825_192642.txt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest results file: results/results_20250825_192642.txt\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Create the dataloaders \n",
    "# Training and Evaluation Pipeline\n",
    "# Output is placed into /results\n",
    "# The best model is saved to /best_model\n",
    "###############################################################################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score as f1_score_sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "from linformer import Linformer\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import gc  # Add garbage collection\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear memory cache for MPS/CUDA\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "\n",
    "def generate_classification_report(all_labels, all_preds):\n",
    "    # Generate the classification report as a dictionary\n",
    "    target_names = [\"Normal\", \"Anomalous\"]  # Adjust based on your dataset's classes\n",
    "    report_dict = classification_report(\n",
    "        all_labels, \n",
    "        all_preds, \n",
    "        target_names=target_names, \n",
    "        output_dict=True,\n",
    "        zero_division=0  # avoid warning for undefined metrics\n",
    "        )\n",
    "\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "    # Format numeric columns to desired precision\n",
    "    report_df['precision'] = report_df['precision'].apply(lambda x: f\"{x:.5f}\")\n",
    "    report_df['recall'] = report_df['recall'].apply(lambda x: f\"{x:.5f}\")\n",
    "    report_df['f1-score'] = report_df['f1-score'].apply(lambda x: f\"{x:.5f}\")\n",
    "    report_df['support'] = report_df['support'].apply(lambda x: int(x) if not pd.isna(x) else \"\")  # Format support as integer\n",
    "\n",
    "    # Remove support for the accuracy row\n",
    "    report_df.loc['accuracy', 'support'] = np.nan  # Use NaN for missing numeric values\n",
    "\n",
    "    print(\"Formatted Classification Report:\")\n",
    "    print(report_df)\n",
    "\n",
    "    # Optionally, save the report to a CSV or text file\n",
    "    report_df.to_csv('classification_report.csv', index=True)\n",
    "    return report_df\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, scheduler, criterion, num_epochs=10, patience=3, device='cpu'):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.criterion = criterion\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        self.total_time = 0\n",
    "        self.timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.best_macro_f1 = 0.0\n",
    "        self.best_marco_acc = 0.0\n",
    "        \n",
    "\n",
    "        # Create necessary folders\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        os.makedirs('best_model', exist_ok=True)\n",
    "\n",
    "    def train(self):\n",
    "        best_val_loss = float('inf')\n",
    "        best_val_f1 = 0.0\n",
    "        patience_counter = 0   \n",
    "        results_file = os.path.join('results', f'results_{self.timestamp}.txt')\n",
    "        best_model_file = os.path.join('best_model', f'best_model_{self.timestamp}.pth')\n",
    "        total_params, size_mb = compute_model_size(model)\n",
    "        total_time_minutes = 0 \n",
    "        memory_initial = 0\n",
    "\n",
    "        # Clear memory before training\n",
    "        clear_memory()\n",
    "\n",
    "        with open(results_file, 'w') as f:\n",
    "            f.write(\"Experiment Parameters:\\n\")\n",
    "            f.write(f\"log_file: {log_file}\\n\")\n",
    "            f.write(f\"windows_size: {windows_size}\\n\")\n",
    "            f.write(f\"step_size: {step_size}\\n\")\n",
    "            f.write(f\"train_ratio: {train_ratio}\\n\")\n",
    "            f.write(f\"beta: {beta}\\n\")\n",
    "            f.write(f\"batch_size: {batch_size}\\n\")\n",
    "            f.write(f\"dropout: {dropout}\\n\")\n",
    "            f.write(f\"num_layers: {num_layers}\\n\")\n",
    "            f.write(f\"num_heads: {num_heads}\\n\")\n",
    "            f.write(f\"k: {k}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\"Total Parameters: {total_params}\\n\")\n",
    "            f.write(f\"Model Size: {size_mb:.2f} MB\\n\")\n",
    "            memory_initial = measure_memory()\n",
    "            epoch_run = 0\n",
    "            for epoch in range(self.num_epochs):\n",
    "                start_time = time.time()\n",
    "                memory_before = measure_memory()\n",
    "                epoch_run +=1\n",
    "                msg = f\"Epoch {epoch + 1}/{self.num_epochs}\"\n",
    "                print(msg)\n",
    "                print(msg, file=f)\n",
    "\n",
    "                # Training Phase\n",
    "                train_loss, train_acc = self._train_one_epoch()\n",
    "                epoch_time_minutes = (time.time() - start_time) / 60\n",
    "                total_time_minutes += epoch_time_minutes\n",
    "\n",
    "                memory_after = measure_memory()\n",
    "                if memory_before == memory_after:\n",
    "                    memory_before = memory_initial\n",
    "\n",
    "                # Validation Phase\n",
    "                val_loss, val_acc, macro_f1, Anomalous_val_precision, Anomalous_val_recall, Anomalous_val_f1, report_df = self._validate()\n",
    "\n",
    "                # Log and Save Metrics\n",
    "                self.train_losses.append(train_loss)\n",
    "                self.train_accuracies.append(train_acc)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accuracies.append(val_acc)\n",
    "\n",
    "            \n",
    "                # Log to console and file\n",
    "                log = (\n",
    "                    f\"Train Loss: {float(train_loss):.5f}, Train Accuracy: {float(train_acc):.5f}\\n\"\n",
    "                    f\"Val Loss: {float(val_loss):.5f}, Val Accuracy: {float(val_acc):.5f}\\n\"\n",
    "                    f\"Anomalous Precision: {float(Anomalous_val_precision):.5f}, \"\n",
    "                    f\"Anomalous Recall: {float(Anomalous_val_recall):.5f}, \"\n",
    "                    f\"Anomalous F1 Score: {float(Anomalous_val_f1):.5f}\\n\"\n",
    "                    f\"Training time for one epoch: {float(epoch_time_minutes):.2f} minutes\\n\"\n",
    "                    f\"The Memory initial allocated beform training: {memory_initial:.2f} MB\\n\"\n",
    "                    f\"Memory Used for training in this epoch: {memory_after - memory_before:.2f} MB\\n\"\n",
    "                )\n",
    "\n",
    "                print(log)\n",
    "                print(log, file=f)\n",
    "\n",
    "                # Log the full classification report to the results file\n",
    "                print(\"\\nClassification Report:\\n\", file=f)\n",
    "                print(report_df.to_string(), file=f)\n",
    "                print(\"------------ END of Epoch --------------\\n\", file=f)\n",
    "\n",
    "                # Scheduler Step\n",
    "                self.scheduler.step(val_loss)\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                msg = f\"Learning Rate: {current_lr:.1e}\"\n",
    "                print(msg)\n",
    "                print(msg, file=f)\n",
    "\n",
    "                # Early Stopping Logic , can based on val_loss or Anmalous_val_f1\n",
    "                if (val_loss < best_val_loss) or (Anomalous_val_f1 > best_val_f1):\n",
    "                    best_val_loss = min(val_loss, best_val_loss)\n",
    "                    best_val_f1 = max(Anomalous_val_f1, best_val_f1)\n",
    "                    self.best_macro_f1 = max(macro_f1, self.best_macro_f1)\n",
    "                    self.best_marco_acc = max(val_acc, self.best_marco_acc)\n",
    "                    patience_counter = 0\n",
    "                    torch.save(self.model.state_dict(), best_model_file)\n",
    "                    msg = f\"New best model saved as: {best_model_file} \\n\"\n",
    "                    print(msg)\n",
    "                    print(msg, file=f)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.patience:\n",
    "                        msg = f\"Early stopping triggered after {patience_counter} epochs without improvement. \\n\"\n",
    "                        print(msg)\n",
    "                        print(msg, file=f)\n",
    "                        break\n",
    "            \n",
    "            # average training time per epoch \n",
    "            avg_time_minutes = total_time_minutes / epoch_run\n",
    "            msg = f\"Average Training Time per Epoch: {avg_time_minutes} min\"\n",
    "            print(msg)\n",
    "            print(msg, file=f)\n",
    "\n",
    "            # peak GPU memory allocated\n",
    "            if torch.cuda.is_available():\n",
    "                msg = f\"Peak GPU Memoryy Allocated: {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\"\n",
    "                print(msg)\n",
    "                print(msg, file=f)\n",
    "            # Plot Training History\n",
    "            self._plot_training_history()\n",
    "\n",
    "    def _train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loop = tqdm(self.train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for batch in loop:\n",
    "            input_ids, segment_ids, attention_masks, labels = [b.to(self.device) for b in batch]\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            logits = self.model(input_ids, segment_ids, attention_masks)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            #----------addind for reduce overfitting ---------------\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            #--------------------------------------------------------\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), accuracy=correct / total)\n",
    "\n",
    "        return total_loss / len(self.train_loader), correct / total\n",
    "\n",
    "    def _validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        loop = tqdm(self.val_loader, desc=\"Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch in loop:\n",
    "                input_ids, segment_ids, attention_masks, labels = [b.to(self.device) for b in batch]\n",
    "                logits = self.model(input_ids, segment_ids, attention_masks)\n",
    "                loss = self.criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                #--for debug, show the partial f1 score---\n",
    "                # Compute partial F1 with current predictions\n",
    "                partial_f1 = f1_score_sklearn(all_labels, all_preds, pos_label=1,zero_division=0)\n",
    "                loop.set_postfix(partial_f1=f\"{partial_f1:.5f}\", loss=loss.item())\n",
    "                \n",
    "        # Generate formatted classification report\n",
    "        report_df = generate_classification_report(all_labels, all_preds)\n",
    "        # Extract specific metrics\n",
    "        if \"Anomalous\" in report_df.index:\n",
    "            precision = float(report_df.loc[\"Anomalous\", \"precision\"])\n",
    "            recall = float(report_df.loc [\"Anomalous\", \"recall\"])\n",
    "            f1_score = float(report_df.loc[\"Anomalous\", \"f1-score\"])\n",
    "        else:\n",
    "            precision, recall, f1_score = 0.0, 0.0, 0.0\n",
    "\n",
    "        \n",
    "        # Overall f1-score from report_df\n",
    "        if \"macro avg\" in report_df.index:\n",
    "            macro_f1 = float(report_df.loc[\"macro avg\", \"f1-score\"])\n",
    "        else:\n",
    "            macro_f1 = 0.0\n",
    "\n",
    "        # Overall accuracy\n",
    "        accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "\n",
    "        return total_loss / len(self.val_loader), accuracy,macro_f1, precision, recall, f1_score, report_df\n",
    "\n",
    "    def measure_inference_time(self, input_sample, num_runs=100):\n",
    "        \"\"\"measure inference time and memory usage\"\"\"\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        input_sample = [tensor.to(self.device) for tensor in input_sample]\n",
    "        with torch.no_grad():\n",
    "            # measure memory before inference\n",
    "            memory_before = measure_memory()\n",
    "\n",
    "            for _ in range(10):    # warm up befor actual benchmarking \n",
    "                _ = self.model(*input_sample)\n",
    "            start_time = time.time()\n",
    "            for _ in range(num_runs):\n",
    "                _ = self.model(*input_sample)\n",
    "            total_time = time.time() - start_time\n",
    "            # measure memory after inference\n",
    "            memory_after = measure_memory()\n",
    "\n",
    "        avg_inference_time = (total_time / num_runs) * 1000  # Convert to milliseconds\n",
    "        print(f\"Average Inference Time: {avg_inference_time:.2f} ms\")\n",
    "        mem_during_inference = memory_after - memory_before\n",
    "        print(f\"Memory Used During Inference: {mem_during_inference:.2f} MB\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            GPU_max_mem_during_inference = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "            print(f\"Peak GPU Memory Allocated: {GPU_max_mem_during_inference:.2f} MB\")\n",
    "        return avg_inference_time, mem_during_inference, GPU_max_mem_during_inference\n",
    "\n",
    "    def _plot_training_history(self):\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        training_graph = os.path.join('results', f'training_history_{self.timestamp}.png')\n",
    "\n",
    "        # Plot Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.train_losses, label='Train Loss')\n",
    "        plt.plot(epochs, self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, self.train_accuracies, label='Train Accuracy')\n",
    "        plt.plot(epochs, self.val_accuracies, label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(training_graph)\n",
    "        plt.show()\n",
    "\n",
    "# collate_fn for dataloader to handle variable length of session of logs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for training_sessions.\n",
    "\n",
    "    Args:\n",
    "    - batch: List of dictionaries with keys \"input_ids\", \"segment_ids\", \"session_label\".\n",
    "\n",
    "    Returns:\n",
    "    - input_ids: Padded tensor of shape (batch_size, max_seq_length).\n",
    "    - segment_ids: Padded tensor of shape (batch_size, max_seq_length).\n",
    "    - attention_masks: Tensor of shape (batch_size, max_seq_length).\n",
    "    - session_labels: Tensor of shape (batch_size).\n",
    "    \"\"\"\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"], dtype=torch.long) for item in batch]\n",
    "    segment_ids = [torch.tensor(item[\"segment_ids\"], dtype=torch.long) for item in batch]\n",
    "    session_labels = torch.tensor([item[\"session_label\"] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    padded_segment_ids = pad_sequence(segment_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Clamp padded sequences to max_token_length after padding\n",
    "    padded_input_ids = padded_input_ids[:, :max_token_length]\n",
    "    padded_segment_ids = padded_segment_ids[:, :max_token_length]\n",
    "\n",
    "    \n",
    "    # Create attention masks: 1 for non-padding, 0 for padding\n",
    "    attention_masks = (padded_input_ids != 0).long()\n",
    "\n",
    "    \n",
    "\n",
    "    return padded_input_ids, padded_segment_ids, attention_masks, session_labels\n",
    "\n",
    "def compute_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_mb = total_params * 4 /(1024**2) # assuming 32 bits floating points\n",
    "    return total_params, size_mb\n",
    "\n",
    "def measure_memory():\n",
    "    \"\"\"Measure GPU and CPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / (1024 ** 2)  # Convert to MB\n",
    "    else:\n",
    "        return psutil.Process().memory_info().rss / (1024 ** 2)  # RAM in MB\n",
    "\n",
    "\n",
    "def get_latest_result_file(results_dir=\"results\"):\n",
    "    \"\"\"Get the most recent output file in the results folder.\"\"\"\n",
    "    list_of_files = glob.glob(os.path.join(results_dir, \"results_*.txt\"))  # Get all result files\n",
    "    if not list_of_files:\n",
    "        print(\"No result files found.\")\n",
    "        return None\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)  # Get the newest file\n",
    "    print(f\"Latest results file: {latest_file}\")\n",
    "    return latest_file\n",
    "\n",
    "def load_best_model(model_path, model_class, k, device):\n",
    "    \"\"\"\n",
    "    Load the best saved model from disk.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model.\n",
    "        model_class (torch.nn.Module): Model class to instantiate.\n",
    "        device (str): Device to load the model onto.\n",
    "    \n",
    "    Returns:\n",
    "        model (torch.nn.Module): Loaded model with trained weights.\n",
    "    \"\"\"\n",
    "    model = model_class(\n",
    "            vocab_size=cl100k_vocab_size,  # GPT-4 BPE vocab size\n",
    "            segment_vocab_size=windows_size,  # For BGL and Thunderbird\n",
    "            max_seq_len=max_token_length,\n",
    "            embedding_dim=embedding_dim, num_layers=num_layers, \n",
    "            num_heads=num_heads, k=k, ff_hidden_dim=ff_hidden_dim, num_classes=2,   \n",
    "            dropout=dropout\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    model.eval()\n",
    "    print(f\"Loaded best model from {model_path}\")\n",
    "    return model\n",
    "\n",
    "# Label Smoothing Loss Implementation\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        \"\"\"\n",
    "        smoothing: amount of smoothing (e.g., 0.1 means 10% smoothing)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(-1)\n",
    "        # Convert target to one-hot encoding with smoothing\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logits)\n",
    "            true_dist.fill_(self.smoothing / (num_classes - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        return torch.mean(torch.sum(-true_dist * F.log_softmax(logits, dim=-1), dim=-1))\n",
    "\n",
    "# train and validate \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "#max_token_length = length_95_percentile  # 95% of the sessions are under 969 tokens long\n",
    "max_token_length = max_token_length # 2756 for BGL.log\n",
    "\n",
    "batch_size = 8\n",
    "dropout = 0.5     \n",
    "embedding_dim = 128\n",
    "ff_hidden_dim = 128\n",
    "num_layers = 1\n",
    "num_heads = 4\n",
    "num_epochs = 50 # 50 for BGL evualtion\n",
    "seed = 42  # Set a seed for reproducibility\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    testset_results=[]\n",
    "    k_valuse = [32,]  \n",
    "\n",
    "    for k in k_valuse: \n",
    "        set_seed(seed)\n",
    "        print(f\"running with k = {k}\") \n",
    "        \n",
    "        model = AllLinLog(\n",
    "            vocab_size=cl100k_vocab_size,  # GPT-4 BPE vocab size\n",
    "            segment_vocab_size=windows_size,  # For BGL and Thunderbird\n",
    "            max_seq_len=max_token_length,\n",
    "            embedding_dim=embedding_dim, num_layers=num_layers, \n",
    "            num_heads=num_heads, k=k, ff_hidden_dim=ff_hidden_dim, num_classes=2,   \n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        total_params, size_mb = compute_model_size(model)\n",
    "        print(f\"Total Parameters: {total_params}\")\n",
    "        print(f\"Model Size: {size_mb:.2f} MB\")\n",
    "\n",
    "        #optimizer = optim.Adam(model.parameters(), lr=5e-4)   \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2, eps=1e-8) # validataiton get improved after this change, lr 5e-4\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20)\n",
    "\n",
    "        # criterion = nn.CrossEntropyLoss()\n",
    "        class_weights = torch.tensor([1.0, 4.0]).to(device) # Adjust weights for class imbalance, 1.0 for normal, 4.0 for anomalous\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        train_dataset = LogDataset(training_sessions)\n",
    "        val_dataset = LogDataset(val_sessions)\n",
    "        test_dataset = LogDataset(test_sessions)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            collate_fn=collate_fn, \n",
    "            shuffle=True, \n",
    "            pin_memory=True  # Optimize data transfer to GPU\n",
    "            )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            collate_fn=collate_fn, \n",
    "            pin_memory=True  # Optimize data transfer to GPU\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            collate_fn=collate_fn, \n",
    "            pin_memory=True  # Optimize data transfer to GPU\n",
    "        )\n",
    "\n",
    "\n",
    "        # Verify data\n",
    "        for batch in train_loader:\n",
    "            input_ids, segment_ids, attention_masks, session_labels = batch\n",
    "            print(f\"Input IDs: {input_ids.shape}, Segment IDs: {segment_ids.shape}\")\n",
    "            print(f\"Attention Masks: {attention_masks.shape}, Session Labels: {session_labels.shape}\")\n",
    "            break\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            criterion=criterion,\n",
    "            num_epochs=num_epochs,  \n",
    "            patience=30,   \n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # After training, evaluate the best model on the test set for unbiased metrics\n",
    "        def evaluate_on_test_set(trainer, test_loader, dataset_name=None):\n",
    "            trainer.model.eval()\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(test_loader, desc=\"Evaluating on Test Set\", leave=False):\n",
    "                    input_ids, segment_ids, attention_masks, labels = [b.to(trainer.device) for b in batch]\n",
    "                    logits = trainer.model(input_ids, segment_ids, attention_masks)\n",
    "                    preds = logits.argmax(dim=1)\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "            report_df = generate_classification_report(all_labels, all_preds)\n",
    "            # Save the test evaluation result to a CSV file with timestamp and dataset name if provided, in the 'results' folder\n",
    "            dataset_str = f\"_{dataset_name}\" if dataset_name else \"\"\n",
    "            test_report_filename = os.path.join('results', f'test_classification_report{dataset_str}_{trainer.timestamp}.csv')\n",
    "            report_df.to_csv(test_report_filename, index=True)\n",
    "            # Also save the pretty printed report to a .txt file\n",
    "            txt_report_filename = os.path.join('results', f'test_classification_report{dataset_str}_{trainer.timestamp}.txt')\n",
    "            with open(txt_report_filename, 'w') as f:\n",
    "                f.write(\"Unbiased Test Set Classification Report:\\n\\n\")\n",
    "                f.write(report_df.to_string())\n",
    "            print(f\"Test evaluation result saved to {test_report_filename} and {txt_report_filename}\")\n",
    "            return report_df\n",
    "\n",
    "        # Load the best model before test evaluation\n",
    "        best_model_path = os.path.join('best_model', f'best_model_{trainer.timestamp}.pth')\n",
    "        trainer.model.load_state_dict(torch.load(best_model_path, map_location=trainer.device))\n",
    "        trainer.model.eval()\n",
    "\n",
    "        # Now evaluate on the test set\n",
    "        test_report_df = evaluate_on_test_set(trainer, test_loader, dataset_name='BGL')\n",
    "        \n",
    "        # Extract test set metrics\n",
    "        test_macro_f1 = float(test_report_df.loc[\"macro avg\", \"f1-score\"])\n",
    "        test_accuracy = float(test_report_df.loc[\"accuracy\", \"f1-score\"])  # accuracy is stored in f1-score column for accuracy row\n",
    "\n",
    "        testset_results.append({\n",
    "            \"k\": k,\n",
    "            \"macro_f1\": test_macro_f1,\n",
    "            \"test_acc\": test_accuracy,\n",
    "        })\n",
    "\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(testset_results)\n",
    "results_df.to_csv(\"ablation_k_results.csv\", index=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results_df[\"k\"], results_df[\"macro_f1\"], color='black', linestyle='-', marker='o', label=\"F1-score\")\n",
    "plt.plot(results_df[\"k\"], results_df[\"test_acc\"], color='black', linestyle='--', marker='x', label=\"Accuracy\")\n",
    "\n",
    "plt.xscale(\"log\", base=2)\n",
    "plt.xticks(results_df[\"k\"], labels=results_df[\"k\"])\n",
    "\n",
    "plt.xlabel(\"Projected Dimension k\")\n",
    "plt.ylabel(\"Score\")\n",
    "#plt.title(\"AllLinLog: Effect of k on Performance\")\n",
    "plt.grid(True, linestyle=':', color='gray')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ablation_plot_macro_f1_acc_bw.png\")\n",
    "plt.show()\n",
    "\n",
    "# Append output summary to the most recent result file\n",
    "result_file = get_latest_result_file()\n",
    "if result_file:\n",
    "    with open(result_file, 'a') as f:\n",
    "        f.write(\"\\nAblation Study Completed\\n\")\n",
    "        f.write(\"Saved CSV: ablation_k_results.csv\\n\")\n",
    "        f.write(\"Saved Plot: ablation_plot_macro_f1_acc.png\\n\")\n",
    "        f.write(\"Results Summary:\\n\")\n",
    "        f.write(results_df.to_string(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"End of Ablation Study\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
